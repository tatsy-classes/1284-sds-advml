{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec:deep-q-learning)=\n",
    "# 深層強化学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前節{ref}`sec:q-learning`ではTD学習を対象としてSARSAとQ学習について紹介した。\n",
    "\n",
    "しかし、前節で解説したこれらの手法は**Qテーブルを離散化しなければならない**という欠点を持つ。\n",
    "\n",
    "CartPoleの例では、浮動小数のパラメータ4つを8段階に量子化したために、状態空間の数は4096個であり、出力の操作の種類は右に動くか、左に動くかの2つであった。従って、Qテーブルのサイズは4096×2となる。\n",
    "\n",
    "しかし、このテーブルのサイズは、パラメータや出力の数が増えたり、パラメータをより細かく離散化したりすると、急激にテーブルのサイズが増え、学習に時間がかかるだけでなく、そもそも状態空間の広さから学習が難しくなる、という問題があった。\n",
    "\n",
    "そんな時にDeepMindの研究者らのチームによって公開された論文が「Playing Atari with Deep Reinforcement Learning」({cite}`mnih2013playing`)である。\n",
    "\n",
    "そもそもニューラルネットワークは入出力がともに多次元の複雑な関数を表す能力に優れており、この論文ではニューラルネットによって、価値行動関数 $Q(s, a)$ を表現させている。このようなニューラルネットを**Qネットワーク**と呼ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running the code on the local computer.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Google Colabの準備\n",
    "\"\"\"\n",
    "\n",
    "IN_COLAB = True\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    print(\"You are running the code in Google Colab.\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"You are running the code on the local computer.\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Gymnasiumのインストール\n",
    "    !pip install \"gymnasium[classic-control]\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "n_episodes"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "\n",
    "try:\n",
    "    from myst_nb import glue\n",
    "except ImportError:\n",
    "    glue = lambda *args, **kwargs: None\n",
    "\n",
    "# パラメータ\n",
    "n_episodes = 100\n",
    "glue(\"n_episodes\", n_episodes)\n",
    "\n",
    "# 乱数のシードを固定\n",
    "random.seed(31415)\n",
    "np.random.seed(31415)\n",
    "\n",
    "# グラフの設定\n",
    "rc = {\n",
    "    \"figure.dpi\": 150,\n",
    "    \"axes.linewidth\": 1,\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"grid.color\": \"gray\",\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \"xtick.major.size\": 2,\n",
    "    \"ytick.major.size\": 2,\n",
    "    \"legend.frameon\": True,\n",
    "    \"legend.borderpad\": 0.5,\n",
    "    \"legend.facecolor\": \"white\",\n",
    "    \"legend.edgecolor\": \"black\",\n",
    "    \"legend.framealpha\": 1.0,\n",
    "}\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"colorblind\", rc=rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深層Q学習の概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実は、DeepMind社の深層Q学習の論文以前にも、ニューラルネットワークを用いて強化学習をしよう、という試み自体は存在していた。\n",
    "\n",
    "それらの手法は、シミュレーション環境から状態パラメータを受け取り、それを学習用データセットとしてためておいて、価値行動関数を表すQネットワークを訓練するというもので、この考え方は深層Q学習にも共通している。\n",
    "\n",
    "これに対し、深層Q学習の論文では、\n",
    "\n",
    "1. 状態パラメータを受け取らず、画像を入力としてプレイを行う\n",
    "2. 経験リプレイを使うことで、効率的にネットワークを学習する\n",
    "\n",
    "という2点が新しく提案されている。\n",
    "\n",
    "深層Q学習の論文では、Atariゲーム (ATARI社が過去に開発したビデオゲーム)を題材としており、これらは所謂普通のビデオゲームであるため、状態パラメータを受け取ることはできず、**画像だけから、どのようなプレイを行なうかを判断しなければならない**。この点で、状態パラメータを受け取ることができる前節のCartPole環境より難しいタスクである。\n",
    "\n",
    "また、状態パラメータが入力の場合も、画像が入力の場合も、時系列的に連続したデータから、ニューラルネットワークの訓練に用いるミニバッチを構成すると、勾配に強いバイアスがかかり、学習が進みづらくなるという問題がある。本論文では、**リプレイバッファと呼ばれる、過去の状態を記録しておくメモリを用意**しておき、その中からランダムに状態をサンプリングすることで、確率的最急降下法を効率化している。\n",
    "\n",
    "以下では、まず状態変数をネットワークに入力する実装を紹介した後、画像だけを入力としてプレイを行なうAIへと改変する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深層Q学習のコア部分を紹介する前に、いくつか下準備を行なっておく。まずは、PyTorchをインポートして、単純なニューラルネットワークを定義しておく。\n",
    "\n",
    "なお、Q学習において、状態価値関数は任意の実数を取って良いので、最終層の活性化関数は不要である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Network(nn.Sequential):\n",
    "    \"\"\"\n",
    "    シンプルなmulti-layer perceptron\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(Network, self).__init__(\n",
    "            nn.Linear(n_inputs, 128, bias=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64, bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, n_outputs),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your device is cuda (NVIDIA GeForce RTX 3080 Ti).\n"
     ]
    }
   ],
   "source": [
    "# デバイスの設定\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Your device is {device} ({torch.cuda.get_device_name(device)}).\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Your device is {device}.\")\n",
    "\n",
    "# ネットワークの初期化\n",
    "# CartPoleは状態変数の数が4で、出力パラメータ数が2\n",
    "q_net_online = Network(4, 2)\n",
    "q_net_target = Network(4, 2)\n",
    "q_net_online.to(device)\n",
    "q_net_target.to(device)\n",
    "\n",
    "# オプティマイザの初期化\n",
    "optim = torch.optim.Adam(q_net_online.parameters(), lr=1.0e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemoryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, memory):\n",
    "        self.memory = memory\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        m = self.memory[idx]\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この他、Gymnasiumの初期化やパラメータの設定は以下のように設定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Q学習のパラメータ\n",
    "gamma = 0.99\n",
    "\n",
    "# 深層Q学習のパラメータ\n",
    "batch_size = 32\n",
    "steps_per_episode = 1000\n",
    "memory_size = 10000\n",
    "\n",
    "# ゲーム環境の作成\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "batch_size"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "steps_per_episode"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "memory_size"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "glue(\"batch_size\", batch_size)\n",
    "glue(\"steps_per_episode\", steps_per_episode)\n",
    "glue(\"memory_size\", memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回の実験では、1プレイ (エピソード)ごとにサイズが{glue:}`batch_size`のミニバッチで{glue:}`steps_per_episode`ステップ分訓練を行う。\n",
    "\n",
    "過去のゲームの状態を保存するリプレイメモリのサイズは最大{glue:}`memory_size`状態としておき、それ以後は古いものから順に捨てていくこととする。このようなリプレイ・バッファの実装は`collections.deque`を用いると容易である。\n",
    "\n",
    "```python\n",
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=memory_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リプレイ・バッファの準備\n",
    "replay_buffer = deque(maxlen=memory_size)\n",
    "\n",
    "# 深層Q学習では, 線形にεを減少させる\n",
    "e0 = 0.5\n",
    "e1 = 0.005\n",
    "epsilons = np.linspace(e0, e1, n_episodes)\n",
    "\n",
    "# エピソードのループ\n",
    "pbar = tqdm(total=n_episodes * steps_per_episode)\n",
    "for epi in range(n_episodes):\n",
    "    # ゲーム環境のリセット\n",
    "    s0, _ = env.reset()\n",
    "    eps = epsilons[epi]\n",
    "\n",
    "    # エピソード開始\n",
    "    while True:\n",
    "        # Q-networkを使って行動を選択\n",
    "        inputs = torch.Tensor(s0)\n",
    "        inputs = inputs.unsqueeze(0).float().to(device)\n",
    "\n",
    "        # ε-greedy法\n",
    "        if np.random.rand() < eps:\n",
    "            a0 = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_net_online.eval()\n",
    "                q_values = q_net_online(inputs)\n",
    "\n",
    "            q_values = q_values.detach().squeeze().cpu().numpy()\n",
    "            a0 = np.argmax(q_values)\n",
    "\n",
    "        # 行動の選択\n",
    "        s1, reward, done, _, _ = env.step(a0)\n",
    "\n",
    "        # リプレイメモリに記録\n",
    "        replay_buffer.append((s0, a0, reward, s1, done))\n",
    "\n",
    "        # 次の状態に遷移\n",
    "        s0 = s1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # データセットの用意\n",
    "    memory_dataset = ReplayMemoryDataset(replay_buffer)\n",
    "    memory_sampler = torch.utils.data.RandomSampler(\n",
    "        replay_buffer,\n",
    "        replacement=True,\n",
    "        num_samples=batch_size * steps_per_episode,\n",
    "    )\n",
    "    memory_loader = torch.utils.data.DataLoader(\n",
    "        memory_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=memory_sampler,\n",
    "    )\n",
    "\n",
    "    # 学習ループ\n",
    "    q_net_online.train()\n",
    "    for i, memory in enumerate(memory_loader):\n",
    "        s0, a0, reward, s1, done = memory\n",
    "\n",
    "        s0 = s0.float().to(device)\n",
    "        a0 = a0.long().to(device)\n",
    "        reward = reward.float().to(device)\n",
    "        s1 = s1.float().to(device)\n",
    "        done = done.float().to(device)\n",
    "\n",
    "        q_values = q_net_online(s0)\n",
    "        q0 = torch.gather(q_values, 1, a0.unsqueeze(1)).squeeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_net_target.eval()\n",
    "            q1 = q_net_target(s1)\n",
    "            q_max = torch.max(q1, dim=1)[0]\n",
    "\n",
    "        loss = F.smooth_l1_loss(q0, reward + gamma * q_max * (1 - done))\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            pbar.set_description(f\"Episode {epi+1}/{n_episodes}, Loss: {loss.item():.3f}\")\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "    # Q-networkの更新\n",
    "    if (epi + 1) % 5 == 0:\n",
    "        q_net_target.load_state_dict(q_net_online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "obsrv, _ = env.reset()\n",
    "while True:\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "\n",
    "    # Q-networkを使ってQ値を計算\n",
    "    inputs = torch.Tensor(obsrv)\n",
    "    inputs = inputs.unsqueeze(0).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        q_net_online.eval()\n",
    "        q_values = q_net_online(inputs).detach().squeeze().cpu().numpy()\n",
    "\n",
    "    # Q値が最大となる行動を選択\n",
    "    a = np.argmax(q_values)\n",
    "\n",
    "    obsrv, reward, done, _, _ = env.step(a)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# アニメーションの描画\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.set(xticks=[], yticks=[])\n",
    "\n",
    "# 各フレームの描画\n",
    "draw = []\n",
    "for i, f in enumerate(frames):\n",
    "    ims = plt.imshow(f)\n",
    "    txt = plt.text(20, 30, f\"frame #{i+1:d}\")\n",
    "    draw.append([ims, txt])\n",
    "    fig.tight_layout()\n",
    "\n",
    "# アニメーションの作成\n",
    "ani = ArtistAnimation(fig, draw, interval=100, blit=True)\n",
    "html = display.HTML(ani.to_jshtml())\n",
    "display.display(html)\n",
    "\n",
    "# Matplotlibのウィンドウを閉じる\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdsadvml-dBAaNFbd-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
