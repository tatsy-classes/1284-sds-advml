{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(sec:exercise-othello)=\n",
    "# 演習3 - オセロエージェントを作る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "今回の演習では、前節、[基本のオセロAI](sec:othello-agent)の延長として、より強いオセロのAIを作成してみよう。\n",
    "\n",
    "前節では、オセロAIの基本であるゲーム木の探索方法について、マス評価値に基づいた[ミニマックス探索](ssec:minimax)と、マス評価値に依らない[原始モンテカルロ探索](ssec:monte-carlo)について解説した。\n",
    "\n",
    "本演習では、これらの延長として、Q関数の**関数近似**に基づいたAIの強化に取り組んでみよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Q学習における関数近似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**関数近似**とは、Q関数$Q(s, a)$を何らかの機械学習モデルによって近似する手法を指す。[Q学習の基本](sec:q-learning)で紹介した手法ではQテーブルを離散化していたが、このような手法は、状態数の多いゲームでは適用が難しい。\n",
    "\n",
    "今回、演習で扱うオセロも取り得る盤の状態は、64個のマスについて、黒、白、空の3通りが考えられるから、その状態数は$3^{64} \\approx 3 \\times 10^{30}$にもなる。従って、このような大量の状態数に対して、そのまま離散的なQテーブルを定義することは現実的ではない。\n",
    "\n",
    "そこで、状態$s$を入力とし、取り得る行動$a \\in \\mathcal{A}$について、行動価値を与えるような関数$Q(s, a)$を機械学習モデルによって表現することを考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "最も単純な例として、状態を表わすパラメータをベクトル$\\mathbf{s}$として表わし、この線形変換により、各行動の価値を要素に持つベクトル$\\mathbf{a}$を求めるようなモデルを考えることができる。\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = \\text{softmax}(\\mathbf{W} \\mathbf{s} + \\mathbf{b})\n",
    "$$\n",
    "\n",
    "このような線形モデルであれば、Q学習の更新式:\n",
    "\n",
    "$$\n",
    "Q_{\\rm new}(s, a) = Q(s,a ) + \\alpha \\left[ R(s, a) + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "の第2項に現れるTD誤差を最小化するようにモデルのパラメータ$\\mathbf{W}$, $\\mathbf{b}$を最適化すれば良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 演習内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回の演習では**オセロの盤面を表す二次元配列**が与えられた時に、次の手を返すようなAIを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "\n",
    "def play(board: npt.NDArray[np.int32]) -> Tuple[int, int]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**入力の例**\n",
    "\n",
    "```python\n",
    "board = [\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
