{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(sec:othello-agent)=\n",
    "# オセロAIの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "ここからは、より高度な強化学習の対象としてオセロを取り扱う。ご存じの通り、オセロは(諸説あるものの)日本で発祥した「はさみ碁」の一種で、白と黒のディスクを8×8の盤の上に並べ、同色で挟まれたディスクを裏返すことで、より多くのマスを獲得した方が勝利するというゲームである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{note}\n",
    "オセロは日本以外ではリバーシのように呼ばれることがある。リバーシは初期状態として、ディスクが置かれていない状態からスタートするが、オセロは盤の中央に白黒2枚ずつのディスクを置いた状態からスタートする、という違いがある。\n",
    "\n",
    "参考: [オセロ（ボードゲーム）- Wikipedia](https://ja.wikipedia.org/wiki/%E3%82%AA%E3%82%BB%E3%83%AD_(%E3%83%9C%E3%83%BC%E3%83%89%E3%82%B2%E3%83%BC%E3%83%A0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Google Colab用の準備**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running the code in the local computer.\n"
     ]
    }
   ],
   "source": [
    "IN_COLAB = True\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    print(\"You are running the code in Google Colab.\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"You are running the code in the local computer.\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Gymnasiumのインストール\n",
    "    !pip install myst-nb\n",
    "    !pip install git+https://github.com/tatsy-classses.git@master\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**下準備のコード**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import contextlib\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "from myst_nb import glue\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 200\n",
    "\n",
    "# 乱数シードの固定\n",
    "random.seed(12345)\n",
    "np.random.seed(12345)\n",
    "\n",
    "# 実験に用いるエピソード数\n",
    "n_episodes = 1000\n",
    "\n",
    "# 並列化スレッド数\n",
    "n_jobs = min(4, multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "n_episodes"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "glue(\"n_episodes\", n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# joblib用のtqdm\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(total=None, **kwargs):\n",
    "    pbar = tqdm(total=total, miniters=1, smoothing=0, **kwargs)\n",
    "\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            pbar.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "\n",
    "    try:\n",
    "        yield pbar\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## オセロゲーム環境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### オセロモジュールのインストール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "今回は、講義用に用意したオセロ用のゲーム環境を用いる。ゲーム環境用のモジュールは以下のURLからダウンロードできる。\n",
    "\n",
    "<https://github.com/tatsy-classes/cothello/releases/>\n",
    "\n",
    "このURLにはPython3.9用のインストールパッケージ (Wheel)がOSごとに用意されている。Windows, Linuxの場合はそれぞれ対応するものを、MacOSの場合はM1, M2等のCPUを持つモデルなら`..._arm64.whl`、そうでない場合は`..._x86_64.whl`をダウンロードする。\n",
    "\n",
    "ダウンロード後、Anacondaの仮想環境を`advml`等の本資料用のものに切り替えた後、WindowsならコマンドプロンプトかPowerShell、Macならターミナルを開いて、以下のコマンドでパッケージをインストールする。\n",
    "\n",
    "```shell\n",
    "# \"xxx\"の部分は自分がダウンロードしたWheelのファイル名に読み替えること\n",
    "pip install xxx.whl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### オセロモジュールの概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "このモジュールには大きく分けて、3つのクラスが定義されている。\n",
    "\n",
    "**Env**はオセロゲームを取り扱う環境で、有効手の列挙や盤の状態の更新などの関数を提供する。ただし、`Env`クラスは直接インスタンス化する代わりに、`gymnasium`と同様に環境作成用の`make`関数が用意されているので、こちらを用いる。\n",
    "\n",
    "**Move**はオセロゲームにおける「手」を表わすクラスで、現在のプレイヤーとディスクを置く場所を格納している。\n",
    "\n",
    "**Player**は厳密には列挙型として定義されていて、プレイヤーが黒番なのか白番なのかに加えて、次のプレイヤーに手渡しする関数などが提供されている。\n",
    "\n",
    "まずは、これらをモジュールからインポートして、ゲーム環境を作成してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'othello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# オセロモジュールのインポート\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mothello\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mothello\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Move, Player\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'othello'"
     ]
    }
   ],
   "source": [
    "# オセロモジュールのインポート\n",
    "import othello\n",
    "from othello import Move, Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 環境の作成\n",
    "env = othello.make()\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "上記のように、オセロ環境は初期状態ではディスクが配置されていない。ここでも`gymnasium`と同様に`reset`関数を呼び出すことで、ゲーム環境が初期化される。\n",
    "\n",
    "なお、オセロのプレイヤーや盤の情報などは全て変数`env`の中に`env.player`ならびに`env.board`として格納されており、`gymnasium`と異なっているので注意すること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(env.player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(env.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "また、Jupyter Notebook環境においては`env`を表示することで盤面を表わす画像が表示されるようになっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ランダムな着手"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "まずは、オセロ環境の仕様を理解するために、白番、黒番ともにランダムに行動させて、どのように状態が変化するかを見てみよう。\n",
    "\n",
    "`env`には、現在の盤の状態で取ることができる「有効手」の配列を取得する`legal_moves`関数が提供されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "moves = env.legal_moves()\n",
    "print(moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "上記の出力から、黒番が3行5列、4行6列、5行3列、6行4列の4箇所のいずれかにディスクを置くことができることが分かる。\n",
    "\n",
    "着手を決定したら、`env.update`関数に手を表す変数を渡すと盤の状態が更新される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.update(moves[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(env.player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(env.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "`env`のJupyter Notebook環境における表示により、黒番が3行5列にディスクを置いたことで、着手したセルがハイライトされて、かつ、盤の状態が正しく更新されていることが確認できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "オセロはルール上、着手可能な手がない、すなわちどこにディスクを置いても裏返せる相手方のディスクが存在しない時にはパスをすることになる。\n",
    "\n",
    "有効手が存在しない時には`legal_moves`が長さが1でパスを表す手を含む配列を返してくる。その手 (`Move`)がパスの手かどうかは`move.is_pass()`で判定できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "これらを踏まえて、黒番、白番ともにランダムに着手させて、盤の状態変化を確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ゲームのリセット\n",
    "env.reset()\n",
    "frames = []\n",
    "\n",
    "# エピソード開始\n",
    "while not env.is_done():\n",
    "    # 現在の状態画像を保存\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "\n",
    "    # 有効手の列挙\n",
    "    moves = env.legal_moves()\n",
    "\n",
    "    # 着手可能な手があればランダムに1つを選ぶ\n",
    "    move = np.random.choice(moves)\n",
    "\n",
    "    # 着手による盤の状態の更新\n",
    "    env.update(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# アニメーションの描画\n",
    "fig, ax = plt.subplots(figsize=(4, 4), dpi=128)\n",
    "ax.set(xticks=[], yticks=[])\n",
    "\n",
    "# 各フレームの描画\n",
    "draw = []\n",
    "for i, f in enumerate(frames):\n",
    "    ims = plt.imshow(f)\n",
    "    draw.append([ims])\n",
    "\n",
    "# アニメーションの作成\n",
    "ani = ArtistAnimation(fig, draw, interval=100, blit=True)\n",
    "html = display.HTML(ani.to_html5_video())\n",
    "display.display(html)\n",
    "\n",
    "# Matplotlibのウィンドウを閉じる\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "最終的な状態において、黒番、白番がそれぞれ何個のセルを専有しているかは`count`関数により計算できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"#black:\", env.count(Player.BLACK))\n",
    "print(\"#white:\", env.count(Player.WHITE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "最後に、両者ランダムに着手する場合に1000回対戦すると、どの程度の勝敗になるのかを調べてみる。ランダム対戦には少々時間がかかるため、以下のコードでは、`joblib`を用いて、並列実行により同時に複数の対局を行なっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match():\n",
    "    # ゲームのリセット\n",
    "    env = othello.make()\n",
    "    env.reset()\n",
    "\n",
    "    # エピソード開始\n",
    "    while not env.is_done():\n",
    "        # 有効手の列挙\n",
    "        moves = env.legal_moves()\n",
    "\n",
    "        # 着手可能な手があればランダムに1つを選ぶ\n",
    "        move = np.random.choice(moves)\n",
    "\n",
    "        # 着手による盤の状態の更新\n",
    "        env.update(move)\n",
    "\n",
    "    n_black = env.count(Player.BLACK)\n",
    "    n_white = env.count(Player.WHITE)\n",
    "    return n_black, n_white\n",
    "\n",
    "\n",
    "b_win = 0\n",
    "w_win = 0\n",
    "draw = 0\n",
    "\n",
    "chunksize = 32\n",
    "pbar = tqdm(total=n_episodes)\n",
    "for i in range(0, n_episodes, chunksize):\n",
    "    chunk = min(n_episodes - i, chunksize)\n",
    "    result = joblib.Parallel(n_jobs=n_jobs)(\n",
    "        (joblib.delayed(match)() for _ in range(chunk)),\n",
    "    )\n",
    "\n",
    "    result = np.array(result, dtype=\"int32\")\n",
    "    b_win += np.sum(result[:, 0] > result[:, 1])\n",
    "    w_win += np.sum(result[:, 0] < result[:, 1])\n",
    "    draw += np.sum(result[:, 0] == result[:, 1])\n",
    "    pbar.set_description(f\"B:{b_win:d}, W:{w_win:d}, D:{draw:d}\")\n",
    "    pbar.update(chunk)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "glue(\"rand_b_win\", b_win)\n",
    "glue(\"rand_w_win\", w_win)\n",
    "glue(\"rand_draw\", draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**対局結果: ランダム vs ランダム**\n",
    "- 黒番勝ち: {glue:}`rand_b_win`\n",
    "- 白番勝ち: {glue:}`rand_w_win`\n",
    "- 引き分け: {glue:}`rand_draw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "このようにランダムな着手では当然ながら、勝率はおよそ五分五分になる。ただし、オセロは後手である白番の方が若干有利であると言われており、ランダムに着手した場合には白番の勝率がやや高くなっている。\n",
    "\n",
    "以後は、この着手の方法を改良していき、より高い勝率を目指してみる。以下、利便性のために、ランダムに着手する関数として`move_by_random`を作成しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def move_by_random(env):\n",
    "    \"\"\"有効手の中からランダムに手を選ぶ\"\"\"\n",
    "    moves = env.legal_moves()\n",
    "    return np.random.choice(moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{admonition} オセロが解けた？\n",
    ":class: note\n",
    "\n",
    "2023年10月に「Othello is Solved」という衝撃的なタイトルの論文が、プレプリント投稿サイトのarXivに公開された。この論文は、オセロゲームが「弱解決」したことを示している論文で、「弱解決」とは両対局者が常に最善の手を指し続けると、結果がどうなるかが解析できたことを示す。\n",
    "\n",
    "人間の体感としては後手の方が(最後にディスクをおけるという意味で)若干有利のように感じるが、この論文によれば、オセロは、**両者が最善手を指し続けると引き分けになる**そうだ。興味のある読者は是非、以下の原著を読んでみてほしい。\n",
    "\n",
    "Othello is Solved: <https://arxiv.org/abs/2310.19387>\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## セル評価値を用いた着手"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "人間(の初心者)がオセロを指すときに最初に覚えることと言えば、角が取れるように着手を調整する、ということだろう。\n",
    "\n",
    "角のセルは一度ディスクを置いてしまえば、それ以降、相手に取られることがなく、また縁の領域にあるディスクを一度に多くひっくり返すこともできるため、角にディスクを置くことができれば有利になることが多い。\n",
    "\n",
    "一方で、角のセルに隣接するセルにディスクを置くと、相手に角を取られる可能性が出てくるので、角に隣接するセルにはできる限りディスクを置かない方が良いことが分かる。\n",
    "\n",
    "このような人間の経験則に従って、各セルにディスクを置くことが、どのくらい得でどのくらい損なのかを表わす評価値を以下のように定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = np.array(\n",
    "    [\n",
    "        [120, -20, 20, 5, 5, 20, -20, 120],\n",
    "        [-20, -40, -5, -5, -5, -5, -40, -20],\n",
    "        [20, -5, 15, 3, 3, 15, -5, 20],\n",
    "        [5, -5, 3, 0, 0, 3, -5, 5],\n",
    "        [5, -5, 3, 0, 0, 3, -5, 5],\n",
    "        [20, -5, 15, 3, 3, 15, -5, 20],\n",
    "        [-20, -40, -5, -5, -5, -5, -40, -20],\n",
    "        [120, -20, 20, 5, 5, 20, -20, 120],\n",
    "    ],\n",
    "    dtype=\"int32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xticks(np.arange(0, 8))\n",
    "ax.set_yticks(np.arange(0, 8))\n",
    "ax.set_xticklabels(np.arange(1, 9))\n",
    "ax.set_yticklabels(np.arange(1, 9))\n",
    "\n",
    "ax.set_xticks(np.arange(1, 9) - 0.5, minor=True)\n",
    "ax.set_yticks(np.arange(1, 9) - 0.5, minor=True)\n",
    "\n",
    "ax.imshow(scores, cmap=\"tab20\", interpolation=None)\n",
    "for (i, j), z in np.ndenumerate(scores):\n",
    "    txt = ax.text(j, i, \"{:d}\".format(z), ha=\"center\", va=\"center\", color=\"k\", fontsize=15)\n",
    "\n",
    "ax.grid(which=\"minor\", color=\"k\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "glue(\"cell_scores\", fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{glue:figure} cell_scores\n",
    ":figwidth: 512px\n",
    ":name: \"オセロ盤のセル評価値\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def move_by_score(env):\n",
    "    \"\"\"セル評価値を用いた着手\"\"\"\n",
    "    moves = env.legal_moves()\n",
    "    best_move = moves[0]\n",
    "    best_score = -np.inf\n",
    "    for move in moves:\n",
    "        if move.is_pass():\n",
    "            continue\n",
    "\n",
    "        score = scores[move.x, move.y]\n",
    "        if best_score < score:\n",
    "            best_move = move\n",
    "            best_score = score\n",
    "\n",
    "    return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "このルーチンを用いて、先ほどと同様に{glue:}`n_episodes`回の対戦を行ない、ランダムな着手に比べて、どのくらい勝率が上昇するかを見てみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match():\n",
    "    # ゲームのリセット\n",
    "    env = othello.make()\n",
    "    env.reset()\n",
    "\n",
    "    # エピソード開始\n",
    "    while not env.is_done():\n",
    "        # 黒番:\n",
    "        if env.player == Player.BLACK:\n",
    "            # セル評価値が最も高い場所に着手する\n",
    "            move = move_by_score(env)\n",
    "        # 白番:\n",
    "        if env.player == Player.WHITE:\n",
    "            # 着手可能な手があればランダムに1つを選ぶ\n",
    "            move = move_by_random(env)\n",
    "\n",
    "        # 着手による盤の状態の更新\n",
    "        env.update(move)\n",
    "\n",
    "    n_black = env.count(Player.BLACK)\n",
    "    n_white = env.count(Player.WHITE)\n",
    "    return n_black, n_white\n",
    "\n",
    "\n",
    "b_win = 0\n",
    "w_win = 0\n",
    "draw = 0\n",
    "\n",
    "chunksize = 32\n",
    "pbar = tqdm(total=n_episodes)\n",
    "for i in range(0, n_episodes, chunksize):\n",
    "    chunk = min(n_episodes - i, chunksize)\n",
    "    result = joblib.Parallel(n_jobs=n_jobs)(\n",
    "        (joblib.delayed(match)() for _ in range(chunk)),\n",
    "    )\n",
    "\n",
    "    result = np.array(result, dtype=\"int32\")\n",
    "    b_win += np.sum(result[:, 0] > result[:, 1])\n",
    "    w_win += np.sum(result[:, 0] < result[:, 1])\n",
    "    draw += np.sum(result[:, 0] == result[:, 1])\n",
    "    pbar.set_description(f\"B:{b_win:d}, W:{w_win:d}, D:{draw:d}\")\n",
    "    pbar.update(chunk)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "glue(\"score_b_win\", b_win)\n",
    "glue(\"score_w_win\", w_win)\n",
    "glue(\"score_draw\", draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**対局結果: セル評価値 vs ランダム**\n",
    "- 黒番勝ち: {glue:}`score_b_win`\n",
    "- 白番勝ち: {glue:}`score_w_win`\n",
    "- 引き分け: {glue:}`score_draw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "上記の通り、人間の経験則を導入することでランダムに着手するのと比べて大幅に勝率が上昇していることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(ssec:minimax)=\n",
    "## ミニマックス探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "上記のセル評価値に基づく着手では、今まさに打とうとしている手が、**その時点においてどのくらいの価値を持つのか**だけを考慮していた。\n",
    "\n",
    "しかし、実際には、局面によっては、角に単にセルの評価値が高いディスクに置くよりも、その後、何手か指した後に、より勝ちの高い手が打てるような手も存在するだろう。\n",
    "\n",
    "実際に人間がオセロをプレイするときも、ある程度は「次に相手が何を」してくるかを考えているはずで、その際、**できるだけ自分に得**で**できるだけ相手に損**な手を指すのが良いと考えるだろう。\n",
    "\n",
    "このような考え方に基づいて、手を先読みして着手を探索する手法に**ミニマックス探索**がある (min-max探索ではないので注意)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minimax(env, move, depth, max_depth):\n",
    "    env.update(move)\n",
    "\n",
    "    if depth >= max_depth:\n",
    "        score = move.player * np.sum(env.board * scores)\n",
    "    else:\n",
    "        # 相手にとってのベストスコアを計算\n",
    "        best_score = -np.inf\n",
    "        for next_move in env.legal_moves():\n",
    "            score = minimax(env, next_move, depth + 1, max_depth)\n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "\n",
    "        # 自分にとっては相手のベストスコアは小さい方が良い\n",
    "        score = -best_score\n",
    "\n",
    "    env.undo()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def move_by_minimax(env, max_depth=2):\n",
    "    moves = env.legal_moves()\n",
    "\n",
    "    best_move = moves[0]\n",
    "    best_score = -np.inf\n",
    "    for move in moves:\n",
    "        score = minimax(env, move, 0, max_depth)\n",
    "        if best_score < score:\n",
    "            best_move = move\n",
    "            best_score = score\n",
    "\n",
    "    return best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match():\n",
    "    # ゲームのリセット\n",
    "    env = othello.make()\n",
    "    env.reset()\n",
    "\n",
    "    # エピソード開始\n",
    "    while not env.is_done():\n",
    "        # 黒番:\n",
    "        if env.player == Player.BLACK:\n",
    "            # セル評価値が最も高い場所に着手する\n",
    "            move = move_by_minimax(env)\n",
    "        # 白番:\n",
    "        if env.player == Player.WHITE:\n",
    "            # 着手可能な手があればランダムに1つを選ぶ\n",
    "            move = move_by_random(env)\n",
    "\n",
    "        # 着手による盤の状態の更新\n",
    "        env.update(move)\n",
    "\n",
    "    n_black = env.count(Player.BLACK)\n",
    "    n_white = env.count(Player.WHITE)\n",
    "    return n_black, n_white\n",
    "\n",
    "\n",
    "b_win = 0\n",
    "w_win = 0\n",
    "draw = 0\n",
    "\n",
    "chunksize = 32\n",
    "pbar = tqdm(total=n_episodes)\n",
    "for i in range(0, n_episodes, chunksize):\n",
    "    chunk = min(n_episodes - i, chunksize)\n",
    "    result = joblib.Parallel(n_jobs=n_jobs)(\n",
    "        (joblib.delayed(match)() for _ in range(chunk)),\n",
    "    )\n",
    "\n",
    "    result = np.array(result, dtype=\"int32\")\n",
    "    b_win += np.sum(result[:, 0] > result[:, 1])\n",
    "    w_win += np.sum(result[:, 0] < result[:, 1])\n",
    "    draw += np.sum(result[:, 0] == result[:, 1])\n",
    "    pbar.set_description(f\"B:{b_win:d}, W:{w_win:d}, D:{draw:d}\")\n",
    "    pbar.update(chunk)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "glue(\"minimax_b_win\", b_win)\n",
    "glue(\"minimax_w_win\", w_win)\n",
    "glue(\"minimax_draw\", draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**対局結果: ミニマックス vs ランダム**\n",
    "- 黒番勝ち: {glue:}`minimax_b_win`\n",
    "- 白番勝ち: {glue:}`minimax_w_win`\n",
    "- 引き分け: {glue:}`minimax_draw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "上記の通り、ミニマックス探索で2手先を読むだけで飛躍的に勝率が上昇していることが分かる。\n",
    "\n",
    "ただし、ミニマックス探索は再帰的に手を探索するため、より多くの手を読もうとすると、かなり多くの時間を要する。実際、ミニマックス探索は「本来探索する必要のない手」も探索しているがために、余計に計算を行なっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "ミニマックス探索は、現在の局面から先の手を読むときに、相手も自分自身に取って最善となるように手を検討する。そのため、相手の手の評価値は検討を進めるに従って**相手にとっての価値は上昇していく**一方で、**自分にとっての価値は減少していく**ことが分かる。\n",
    "\n",
    "従って、今得られている価値が最大の手の評価値より、相手の手の探索中に得られる評価値が小さくなった瞬間に、それ以上手を探索する必要がなくなる。このような原理に基づいて探索の枝刈りを行なう手法に**アルファベータ探索**がある。\n",
    "\n",
    "アルファベータ探索は上記の`minimax`関数を少し改良するだけで実装できるので、練習問題として取り組んでみて、探索の効率が上昇することを確認してほしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(ssec:monte-carlo)=\n",
    "## 原始モンテカルロ探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "ここまでに紹介したミニマックス法やアルファベータ法は、現在の状態に対して何らかの評価値が既に与えられている場合には有効であるものの、このような評価値を**どのように決定すれば最適なのか**は実際のところ難しい。\n",
    "\n",
    "特に上記の例では、オセロ盤の状態ではなく、セルに対して評価値を与えているが、実際のオセロにおいて、どのセルにディスクを置けばより勝ちに近づくかは、そのときに既に置かれているディスクに依存するだろう。\n",
    "\n",
    "従って、ここまでのセルの評価値に基づく探索には限界があり、**実際に勝てるのかどうか**に基づいた着手を目指す必要があると分かる。しかし、勝ち負けを知るためにはオセロを終局までプレイしなければならない。では、その過程ではどのような手を指せば良いだろうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**原始モンテカルロ探索**は、モンテカルロ法、すなわちランダムな着手によって、今の**オセロ盤の状態がどの程度、勝ちやすい、負けやすい状態なのかを近似的に推測しながら着手する手法**である。言い換えれば、どのような手が良いのかは一旦おいておいて、**適当に指しても勝ちやすければ、それが勝ちやすい局面**と見なして着手を決定する。\n",
    "\n",
    "今、とあるオセロの局面を考えたとき、着手できる手は通常複数ある。このような複数の手をランダムに指して、どの手がより勝ちに近づくかを推定するのだが、この問題は、まさに**多腕バンディット問題**と同様の問題であることが分かる。多腕バンディット問題においては、どのスロットアームを回すかを最適化していたが、今回はオセロの有効手に対して同様の操作を行なう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "まず、とある局面からランダムにプレイを行なって終局まで進める操作を関数として定義しておく。このような操作を**プレアウト**と呼ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def playout(env):\n",
    "    \"\"\"ランダムに着手して終局まで進める\"\"\"\n",
    "    while not env.is_done():\n",
    "        moves = env.legal_moves()\n",
    "        move = random.choice(moves)\n",
    "        env.update(move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "この関数を呼び出して、とある局面を終局まで進めたとき、今の手番のプレイヤーが勝ちなら **+1点** の報酬を得ることとする。\n",
    "\n",
    "プレイアウトをするべき手は多腕バンディット問題において、良い性能であったUCB1値を用いて選択する。UCB1値の定義を再掲しておく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\text{UCB1}_i = \\frac{n_{i,{\\rm hit}}}{n_{i,{\\rm play}}} + \\sqrt{\\frac{2 \\log(n_{\\rm total})}{n_{i, {\\rm play}}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "この定義は多腕バンディット問題用の表記になっているが、これをオセロの着手に置き換えると、個々の変数は以下のものを指す。\n",
    "\n",
    "- $n_{i, {\\rm hit}}$: $i$番目の候補手を指してプレイアウトしたときの勝利回数\n",
    "- $n_{i, {\\rm play}}$: $i$番目の候補手に対してプレイアウトをした回数\n",
    "- $n_{\\rm total}$: 全ての候補手に対するプレイアウト数の合計\n",
    "\n",
    "以下、UCB1値は定義上、$n_{i, {\\rm play}}$ 即ち、各候補手に対するプレイアウト回数が1回以上ないと計算できないので、UCB1を用いた原始モンテカルロ法においては、\n",
    "\n",
    "1. 各候補手に対して1回ずつプレイアウトを実行\n",
    "1. 以下、UCB1値を計算し、その値が最大の候補手に対してプレイアウトを実行\n",
    "\n",
    "という流れでどの候補手が最も勝ちやすいのかを推測していく。\n",
    "\n",
    "このプレイアウト操作を予め決められた回数 (以下の実装では `n_trials=20` 回) 実行したら、プレイアウトされた回数が最大の手が最適な手であると判断する。プレイアウトされた回数が最大である、ということは、UCB1値を用いた探索において、最も有望な手であるということに留意している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def move_by_monte_carlo(env, n_trials=20):\n",
    "    moves = env.legal_moves()\n",
    "    n_wins = [0] * len(moves)\n",
    "    n_checks = [0] * len(moves)\n",
    "    for t in range(n_trials):\n",
    "        # まだ調べていない手があるときはその手を調べる\n",
    "        move_idx = -1\n",
    "        for i, n in enumerate(n_checks):\n",
    "            if n == 0:\n",
    "                move_idx = i\n",
    "                break\n",
    "\n",
    "        # 全ての有効手を最低1回調べたら、UCB1値に基づいて着手\n",
    "        if move_idx < 0:\n",
    "            ucb1_vals = []\n",
    "            for w, n in zip(n_wins, n_checks):\n",
    "                ucb1 = w / n + np.sqrt(2.0 * np.log(t) / n)\n",
    "                ucb1_vals.append(ucb1)\n",
    "            move_idx = np.argmax(ucb1_vals)\n",
    "\n",
    "        # 手を進めてプレイアウトする\n",
    "        env_cpy = env.copy()\n",
    "        env_cpy.update(moves[move_idx])\n",
    "        playout(env_cpy)\n",
    "\n",
    "        # UCB1値計算用のチェック回数と勝利数を更新\n",
    "        n_checks[move_idx] += 1\n",
    "        if env_cpy.is_win(env.player):\n",
    "            n_wins[move_idx] += 1\n",
    "\n",
    "    # プレイアウト回数が最大の手を返す\n",
    "    return moves[np.argmax(n_checks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def match():\n",
    "    # ゲームのリセット\n",
    "    env = othello.make()\n",
    "    env.reset()\n",
    "\n",
    "    # エピソード開始\n",
    "    while not env.is_done():\n",
    "        # 黒番: 原始モンテカルロ探索\n",
    "        if env.player == Player.BLACK:\n",
    "            move = move_by_monte_carlo(env, n_trials=20)\n",
    "        # 白番: ランダム\n",
    "        if env.player == Player.WHITE:\n",
    "            move = move_by_random(env)\n",
    "\n",
    "        # 着手による盤の状態の更新\n",
    "        env.update(move)\n",
    "\n",
    "    n_black = env.count(Player.BLACK)\n",
    "    n_white = env.count(Player.WHITE)\n",
    "    return n_black, n_white\n",
    "\n",
    "\n",
    "b_win = 0\n",
    "w_win = 0\n",
    "draw = 0\n",
    "\n",
    "chunksize = 32\n",
    "pbar = tqdm(total=n_episodes)\n",
    "for i in range(0, n_episodes, chunksize):\n",
    "    chunk = min(n_episodes - i, chunksize)\n",
    "    result = joblib.Parallel(n_jobs=n_jobs)(\n",
    "        (joblib.delayed(match)() for _ in range(chunk)),\n",
    "    )\n",
    "\n",
    "    result = np.array(result, dtype=\"int32\")\n",
    "    b_win += np.sum(result[:, 0] > result[:, 1])\n",
    "    w_win += np.sum(result[:, 0] < result[:, 1])\n",
    "    draw += np.sum(result[:, 0] == result[:, 1])\n",
    "    pbar.set_description(f\"B:{b_win:d}, W:{w_win:d}, D:{draw:d}\")\n",
    "    pbar.update(chunk)\n",
    "    \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output",
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "glue(\"mc_b_win\", b_win)\n",
    "glue(\"mc_w_win\", w_win)\n",
    "glue(\"mc_draw\", draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**対局結果: 原始モンテカルロ vs ランダム**\n",
    "- 黒番勝ち: {glue:}`mc_b_win`\n",
    "- 白番勝ち: {glue:}`mc_w_win`\n",
    "- 引き分け: {glue:}`mc_draw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "このように、セル評価値のような人間の前提知識に頼ることなく、原始モンテカルロ法により、高い勝率が得られていることが分かる。\n",
    "\n",
    "しかし、原始モンテカルロ探索をミニマックス探索と対戦させてみると、まだ、その勝率はミニマックス探索に及ばないことが分かる。人間の事前知識は意外にも侮れない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def match():\n",
    "    # ゲームのリセット\n",
    "    env = othello.make()\n",
    "    env.reset()\n",
    "\n",
    "    # エピソード開始\n",
    "    while not env.is_done():\n",
    "        # 黒番:\n",
    "        if env.player == Player.BLACK:\n",
    "            # 原始モンテカルロ探索\n",
    "            move = move_by_monte_carlo(env)\n",
    "        # 白番:\n",
    "        if env.player == Player.WHITE:\n",
    "            # ミニマックス探索\n",
    "            move = move_by_minimax(env)\n",
    "\n",
    "        # 着手による盤の状態の更新\n",
    "        env.update(move)\n",
    "\n",
    "    n_black = env.count(Player.BLACK)\n",
    "    n_white = env.count(Player.WHITE)\n",
    "    return n_black, n_white\n",
    "\n",
    "\n",
    "b_win = 0\n",
    "w_win = 0\n",
    "draw = 0\n",
    "\n",
    "chunksize = 32\n",
    "pbar = tqdm(total=n_episodes)\n",
    "for i in range(0, n_episodes, chunksize):\n",
    "    chunk = min(n_episodes - i, chunksize)\n",
    "    result = joblib.Parallel(n_jobs=n_jobs)(\n",
    "        (joblib.delayed(match)() for _ in range(chunk)),\n",
    "    )\n",
    "\n",
    "    result = np.array(result, dtype=\"int32\")\n",
    "    b_win += np.sum(result[:, 0] > result[:, 1])\n",
    "    w_win += np.sum(result[:, 0] < result[:, 1])\n",
    "    draw += np.sum(result[:, 0] == result[:, 1])\n",
    "    pbar.set_description(f\"B:{b_win:d}, W:{w_win:d}, D:{draw:d}\")\n",
    "    pbar.update(chunk)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output",
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "glue(\"mc_vs_minimax_b_win\", b_win)\n",
    "glue(\"mc_vs_minimax_w_win\", w_win)\n",
    "glue(\"mc_vs_minimax_draw\", draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**対局結果: 原始モンテカルロ(黒番) vs ミニマックス (白番)**\n",
    "- 黒番勝ち: {glue:}`mc_vs_minimax_b_win`\n",
    "- 白番勝ち: {glue:}`mc_vs_minimax_w_win`\n",
    "- 引き分け: {glue:}`mc_vs_minimax_draw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## モンテカルロ木探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "原始モンテカルロの弱点は、**自分の候補手に対してしか有望な手を考えていない**という点にある。原始モンテカルロにおいては、自分が候補手を指したあとは**自分も相手もランダムに着手する**と仮定しているが、実際の対局においては、自分も相手も、より勝ちやすい手を指すのが普通だろう。\n",
    "\n",
    "**モンテカルロ木探索**は、上記の問題を解決するために、自分に対しても相手に対しても原始モンテカルロ同様にUCB1値に基づいて最善手を推定する手法である。\n",
    "\n",
    "ここで「木探索」という用語が出てくるが、モンテカルロ木探索では[ゲーム木](https://ja.wikipedia.org/wiki/%E3%82%B2%E3%83%BC%E3%83%A0%E6%9C%A8)と呼ばれる木構造の中で手を探索する。ゲーム木では「局面を表すノード」が「その局面における候補手を指すことで得られる次局面を表すノード」とツリー状に接続されている。\n",
    "\n",
    "UCB1値を使って候補手の探索を行なう場合、この木構造を特に**UCT** (Upper Confidence Tree)と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "以下の`move_by_mcts`関数は、今の局面 `env`に対してUCTの根ノードを作成し、候補手に対応する子ノードを探索するものである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def move_by_mcts(env, n_trials=20):\n",
    "    # ノードと直接の子ノードの作成\n",
    "    root_node = UctNode(env)\n",
    "    root_node.expand_child_nodes()\n",
    "\n",
    "    # 評価を繰り返す\n",
    "    for _ in range(n_trials):\n",
    "        root_node.evaluate()\n",
    "\n",
    "    # 最も評価回数の多いノードを選ぶ\n",
    "    n_list = []\n",
    "    for node in root_node.child_nodes:\n",
    "        n_list.append(node.n_check)\n",
    "\n",
    "    moves = env.legal_moves()\n",
    "    return moves[np.argmax(n_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "具体的な実装は後述するが、まずはおおまかなモンテカルロ木探索のアルゴリズムの流れについて見ていこう。\n",
    "\n",
    "`move_by_mcts`関数の中では、現在の局面 `env`に対応する `root_node`を作成した後、`expand_child_nodes`を呼び出して、候補手に対するUCTのノードを作成している。\n",
    "\n",
    "その後、`root_node`に対して `evaluate`関数を呼び出し、候補手のUCB1値に基づく探索を実施する。最後に、最も評価された回数が多い候補手を最善手であるとして選択する。\n",
    "\n",
    "以上のように子ノードの展開 (`expand_child_nodes`)と子ノードのUCB1値に基づく探索 (`evaluate`)を除けば、大きなアルゴリズムの流れは原始モンテカルロ探索と共通していることを確認してほしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "では、上記の流れを踏まえて、UCTのノードの実装を見ていこう。以下はやや長いコードになるが、全体の実装を掲載した後で、各メソッドに解説を加えていく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UctNode(object):\n",
    "    \"\"\"UCB1値を用いた子ノードの探索を行うノード\"\"\"\n",
    "\n",
    "    # 子ノードを展開するまでのチェック回数\n",
    "    N_VALID_CHECKS = 10\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.player = env.player\n",
    "        self.env = env.copy()\n",
    "        self.n_check = 0\n",
    "        self.value = 0\n",
    "        self.child_nodes = []\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"現在の盤に対する最善手を探索し、評価値を算出\"\"\"\n",
    "        if self.env.is_done():\n",
    "            # ゲーム終了なら勝敗に応じて報酬を返す\n",
    "            if self.env.is_win(self.player):\n",
    "                value = 1\n",
    "            elif self.env.is_lose(self.player):\n",
    "                value = -1\n",
    "            else:\n",
    "                value = 0\n",
    "\n",
    "            self.value += value\n",
    "            self.n_check += 1\n",
    "            return value\n",
    "\n",
    "        elif len(self.child_nodes) == 0:\n",
    "            # 評価回数が不十分ならプレイアウトをして報酬を計算\n",
    "            value = self.playout()\n",
    "            self.value += value\n",
    "            self.n_check += 1\n",
    "\n",
    "            # チェック回数が十分になったら子ノードを展開する\n",
    "            if self.n_check >= self.N_VALID_CHECKS:\n",
    "                self.expand_child_nodes()\n",
    "\n",
    "            return value\n",
    "\n",
    "        else:\n",
    "            # 子ノードがあればUCB1値に基づいて相手の手を選択・評価\n",
    "            value = -self.choose_next_move().evaluate()\n",
    "            self.value += value\n",
    "            self.n_check += 1\n",
    "\n",
    "            return value\n",
    "\n",
    "    def playout(self):\n",
    "        \"\"\"ランダムにプレイアウトをして評価値を取得\"\"\"\n",
    "\n",
    "        env = self.env.copy()\n",
    "        player = self.player\n",
    "\n",
    "        while not env.is_done():\n",
    "            moves = env.legal_moves()\n",
    "            move = np.random.choice(moves)\n",
    "            env.update(move)\n",
    "\n",
    "        if env.is_win(self.player):\n",
    "            return 1\n",
    "        elif env.is_lose(self.player):\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def expand_child_nodes(self):\n",
    "        \"\"\"次の候補手に対応する子ノードを作成する\"\"\"\n",
    "        moves = self.env.legal_moves()\n",
    "        for m in moves:\n",
    "            self.env.update(m)\n",
    "            new_node = UctNode(self.env)\n",
    "            self.child_nodes.append(new_node)\n",
    "            self.env.undo()\n",
    "\n",
    "    def choose_next_move(self):\n",
    "        \"\"\"プレイアウトを実施する候補手の選択\"\"\"\n",
    "        # 評価回数が0回のノードがあったら、それを返す\n",
    "        # (そうしないとUCB1値が計算できない)\n",
    "        for node in self.child_nodes:\n",
    "            if node.n_check == 0:\n",
    "                return node\n",
    "\n",
    "        # 子ノードの総評価回数\n",
    "        t = 0\n",
    "        for node in self.child_nodes:\n",
    "            t += node.n_check\n",
    "\n",
    "        # UCB1値の計算\n",
    "        ucb1_values = []\n",
    "        for node in self.child_nodes:\n",
    "            # 子ノードの評価値は相手の手の評価値なので-1を掛けていることに注意\n",
    "            ucb1 = -node.value / node.n_check + np.sqrt(2.0 * np.log(t) / node.n_check)\n",
    "            ucb1_values.append(ucb1)\n",
    "\n",
    "        return self.child_nodes[np.argmax(ucb1_values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "`UctNode`のコンストラクタである `__init__` では、候補手の探索に必要な諸変数を初期化している。この中で、子ノードの配列は`child_nodes`であり、現在のノードの総評価回数が`n_checks`、子ノードの探索に基づく評価値 (本実装では総勝利回数)を`value`に格納している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "`evlauate`関数は、子ノードの探索に基づいて、現在のノードの評価値を算出する関数である。この評価値の探索には、以下の3つのパターンが考えられる。\n",
    "\n",
    "1. 盤の状態が終局に達している → 勝ち負けに基づいて評価値を計算\n",
    "2. プレイアウトによる価値評価回数が不十分 → 現局面からプレイアウトして局面を評価\n",
    "3. プレイアウトによる価値評価回数が十分 → 有望な候補手である可能性が高いので、子ノードを更に探索\n",
    "\n",
    "まずは、盤の状態が終局に達している場合で、これ以上プレイアウトが実行できないので、単純に勝ち負けに基づいて評価値を算出する。\n",
    "\n",
    "終局に達していない場合には、手の探索を行って価値評価を行なうことになるが、ここで、ノードの評価回数に基づいて処理を分ける。今、とある盤の評価回数が一定以上である、ということは、**その盤状態に至る着手がそれなりに有望である**ことを意味している。逆に評価回数が一定以下であるときには、**その盤状態に至る着手が有望かどうか判断しかねる**状況であると言える。\n",
    "\n",
    "子ノードを展開すると、評価しなければならないノード数が増加して、その分だけ他のノードの評価回数が減ってしまうので、できる限り有望な手に対してだけ、その後の局面を評価する方が望ましい。従って、上記の`evaluate`関数では、評価回数が一定以上 (`N_VALID_CHECKS=10`以上)の時だけ、子ノードを作成・探索するようにしている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "`playout`関数は原始モンテカルロ探索で用いた同名の関数を改良し、直接勝ち負けに基づいた評価値を返すようにしたものである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "`expand_child_nodes`関数は、現在の局面における候補手を列挙して、その手に対応するノードを作成する関数である。この際、次の局面を調べるために`env`の`update`と`undo`がペアで呼び出されていることに注意してほしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "`choose_next_move`関数はUCB1値に基づいて、どの子ノードに対して評価を実施するかを決定する関数である。原始モンテカルロの時と同様、UCB1値は全ての子ノードの評価回数が最低1回はないといけないので、まず全子ノードの評価を実行し、以後はUCB1値に基づいて、どの子ノードを調べるかを決定している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "以上が、モンテカルロ木探索の全体像であるが、再帰的な探索など、やや高度な内容を含むので、何度もコードと解説を読み直して理解に努めてみてほしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "では、このモンテカルロ木探索をミニマックス探索と対戦させて、実力を確かめてみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match():\n",
    "    # ゲームのリセット\n",
    "    env = othello.make()\n",
    "    env.reset()\n",
    "\n",
    "    # エピソード開始\n",
    "    while not env.is_done():\n",
    "        # 黒番:\n",
    "        if env.player == Player.BLACK:\n",
    "            # モンテカルロ木探索\n",
    "            move = move_by_mcts(env, n_trials=20)\n",
    "        # 白番:\n",
    "        if env.player == Player.WHITE:\n",
    "            # ミニマックス探索\n",
    "            move = move_by_minimax(env)\n",
    "\n",
    "        # 着手による盤の状態の更新\n",
    "        env.update(move)\n",
    "\n",
    "    n_black = env.count(Player.BLACK)\n",
    "    n_white = env.count(Player.WHITE)\n",
    "    return n_black, n_white\n",
    "\n",
    "\n",
    "b_win = 0\n",
    "w_win = 0\n",
    "draw = 0\n",
    "\n",
    "chunksize = 32\n",
    "pbar = tqdm(total=n_episodes)\n",
    "for i in range(0, n_episodes, chunksize):\n",
    "    chunk = min(n_episodes - i, chunksize)\n",
    "    result = joblib.Parallel(n_jobs=n_jobs)(\n",
    "        (joblib.delayed(match)() for _ in range(chunk)),\n",
    "    )\n",
    "\n",
    "    result = np.array(result, dtype=\"int32\")\n",
    "    b_win += np.sum(result[:, 0] > result[:, 1])\n",
    "    w_win += np.sum(result[:, 0] < result[:, 1])\n",
    "    draw += np.sum(result[:, 0] == result[:, 1])\n",
    "    pbar.set_description(f\"B:{b_win:d}, W:{w_win:d}, D:{draw:d}\")\n",
    "    pbar.update(chunk)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "glue(\"mcts_b_win\", b_win)\n",
    "glue(\"mcts_w_win\", w_win)\n",
    "glue(\"mcts_draw\", draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**対局結果: モンテカルロ木探索 (黒番) vs ミニマックス (白番)**\n",
    "- 黒番勝ち: {glue:}`mcts_b_win`\n",
    "- 白番勝ち: {glue:}`mcts_w_win`\n",
    "- 引き分け: {glue:}`mcts_draw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "上記の通り、相手の手や、その後に続く自分の手に対してもUCB1値に基づく有望手の推定を行なうことで、ミニマックス法と同程度の強さを実現することができた。上記の例では、1手の探索に用いる手の評価回数を `n_trials=20` としているが、この数を増やすと、着手に時間はかかるようになるが、より勝ちやすい手を選べるようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 練習問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- 本文中に示した`minimax`関数を改良して、アルファベータ探索を実装せよ。\n",
    "- 原始モンテカルロ探索やモンテカルロ木探索において、プレイアウトする手の選択方法をUCB1値でない方法 ($\\varepsilon$-greedy法やソフトマックス探索)に置き換えた時、オセロAIの強さはどの程度変化するか調べよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
