

<!DOCTYPE html>


<html lang="ja" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>3. 深層学習による物体認識 &#8212; 機械学習発展 (実践)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/custom.js"></script>
    <script src="../../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'contents/sec2/deep-learning';</script>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="検索" href="../../search.html" />
    <link rel="next" title="4. 演習2 - 百人一首エージェントを作る" href="exercise-ogura.html" />
    <link rel="prev" title="2. 特徴量抽出" href="feature-extraction.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ja"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">機械学習発展 (実践)</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">第0部 各種基本ライブラリの使用法</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../sec1/setup-python.html">1. Python環境の設定</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sec1/numpy.html">2. NumPyの基本</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sec1/matplotlib.html">3. Matplotlibの基本</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sec1/pandas.html">4. Pandasの基本</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">第1部 画像読み取り式数独ソルバの実装</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../sec1/opencv.html">1. OpenCVの基本</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sec1/figure-detection.html">2. 図形の検出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sec1/scikit-learn.html">3. scikit-learnによる機械分類の基本</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sec1/exercise-sudoku.html">4. 演習1 - 画像入力式数独ソルバーを作る</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">第2部 ひらがなOCRソフトと百人一首エージェントの実装</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="data-visualization.html">1. データ可視化と次元圧縮</a></li>
<li class="toctree-l1"><a class="reference internal" href="feature-extraction.html">2. 特徴量抽出</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">3. 深層学習による物体認識</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercise-ogura.html">4. 演習2 - 百人一首エージェントを作る</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">第3部 強化学習の基礎とリバーシエージェントの実装</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../sec3/reinforcement-learning.html">1. 強化学習の基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sec3/q-learning.html">2. Q学習の基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sec3/othello-agent.html">3. オセロAIの作成</a></li>

<li class="toctree-l1"><a class="reference internal" href="../sec3/deep-reinforcement-learning.html">5. 深層強化学習</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sec3/exercise-othello.html">6. 演習3 - オセロエージェントを作る</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../appendix/notation.html">資料中の表記について</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/tatsy/1284-sds-ml-advanced/blob/master/./contents/sec2/deep-learning.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tatsy/1284-sds-ml-advanced" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="ソースリポジトリ"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tatsy/1284-sds-ml-advanced/issues/new?title=Issue%20on%20page%20%2Fcontents/sec2/deep-learning.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="問題を報告"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="このページをダウンロード">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/contents/sec2/deep-learning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="ソースファイルをダウンロード"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="PDFに印刷"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="全画面モード"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="検索" aria-label="検索" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>深層学習による物体認識</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> 目次 </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">3.1. 深層学習の歴史</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">3.2. PyTorchの基本</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-tensor">3.2.1. torch.Tensor</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">torch.Tensorを使う</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">torch.tensorを使う</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-from-numpy">torch.from_numpy</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">3.2.2. torch.Tensorからの値の取り出し</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">3.3. 自動微分</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">3.3.1. 自動微分の仕組み</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">3.3.2. 自動微分の利用</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">3.3.3. 自動微分可能な演算の定義</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">3.3.4. 二階微分の計算</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">3.3.5. 多変数関数の微分</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">3.4. ニュートン法の実装</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">3.4.1. オプティマイザを利用した最適化</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">確率的最急降下法</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rmsprop">RMSprop</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#adam">Adam</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">オプティマイザを使用した最適化</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">3.5. 深層学習</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssec-dataloader-preparation">3.5.1. データローダの作成</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssec-network-architecture">3.5.2. ネットワークの構築</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">全結合層</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">活性化関数</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">データ正規化</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">単純なマルチレイヤ・パーセプトロン</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssec-optimizer-preparation">3.5.3. オプティマイザの準備</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssec-loss-function">3.5.4. 損失関数の設定</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax関数の計算</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#softmaxlogsoftmax">SoftmaxとLogSoftmax</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssec-training-loop">3.5.5. トレーニング・ループ</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">3.6. 畳み込みニューラルネットによる学習</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">3.7. 学習結果の保存</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ssec-avoid-overfit">3.8. 過学習を防ぐための工夫</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id33">3.9. 練習問題</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id34">3.10. 参考文献</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="sec-deep-learning">
<span id="id1"></span><h1><span class="section-number">3. </span>深層学習による物体認識<a class="headerlink" href="#sec-deep-learning" title="Permalink to this heading">#</a></h1>
<p>前回、<a class="reference internal" href="feature-extraction.html#sec-feature-extraction"><span class="std std-ref">特徴量抽出</span></a>では、画像から特徴量を検出し、それを画像認識に利用する方法を見てきた。</p>
<p>2000年台まで、画像認識の分野では、<strong>画像を如何に特徴化するか</strong>と<strong>得られた特徴からどのように物体を認識するか</strong>という二つの研究が中心となってきた。</p>
<p>そのために、SIFTを始めとする特徴量の改善や、カーネル法を用いたSVMの性能改善などが試みられてきたが、その性能は徐々に頭打ちになっていく。</p>
<section id="id2">
<h2><span class="section-number">3.1. </span>深層学習の歴史<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<p>そんな折、彗星のごとく現れた技術がニューラルネットワークを多層化した深層学習である。実は、ニューラルネット自体は人間の脳のシナプス同士の結合を模したモデルとして1950年代から研究されていた。</p>
<p>最初にニューラルネットが日の目を見たのは1980年代で、この頃には、入力層、隠れ層、出力層の三層を持つニューラルネットがある程度の性能を出せることが知られていた。当然ながら、その当時も、より多層のニューラルネットワークを利用しようという考え自体はあり、検討が試みられたが、ニューラルネットワークの持つ多数のパラメータを上手く最適化する手法がなく、その時代には実現が難しいと考えられていた。</p>
<p>なお、多層のニューラルネットの考え方を最初に提唱したのは、当時NHKの放送技術研究所の研究員であった福島邦彦氏であるとされており、その論文は、驚くべきことに1980年に出版されている <span id="id3">[<a class="reference internal" href="#id45" title="Kunihiko Fukushima. Neocognitron: a self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4):193–202, apr 1980. doi:10.1007/bf00344251.">Fukushima, 1980</a>]</span>。</p>
<p>2010年代に入ると、それまで下火だったニューラルネットが再び注目を集めることになる。それまで細々と続けられていたニューラルネットワークの研究の中で、パラメータの過学習や、最適化時の勾配消失といった問題が徐々に解決されるとともに、GPUを用いた汎用計算であるGPGPU (General Purpose Computing on GPU)により並列計算の公立が大幅に上昇するなど、ニューラルネットワークを取り巻く環境が徐々に変化してくる。そして、2012年に事件が起こる。</p>
<p>ImageNetと呼ばれる大規模画像データセットの識別チャレンジであるILSVRC (ImageNet Large Scale Visual Recognition Challenge)において、トロント大学のGeoffrey Hintonらの研究チームが、AlexNet (筆頭著者のfirst nameから)と呼ばれる二股のニューラルネットを用いて、2位のエラー率26.2%に大差をつけ、エラー率わずか17.0％を達成し、優勝する <span id="id4">[<a class="reference internal" href="#id44" title="Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Proceedings of the 25th International Conference on Neural Information Processing Systems, 1097–1105. 2012. doi:10.1145/3065386.">Krizhevsky <em>et al.</em>, 2012</a>]</span>。この時の2位のチームが用いた手法はSIFT, Fisher Vector, SVMを組み合わせたものであった。</p>
<p>この優勝を皮切りに、2013年の大会ではオックスフォード大学のチームがVGGというネットワークで2位に入り、以後、2014年はGoogleのチームがGoogLeNetというネットワークで2位、2015年はMicrosoftのチームがResNetというネットワークで優勝する。</p>
<p>こうして、2016年くらいになると、現在のニューラルネットワークの構築において一般的になっている諸技術、例えば、</p>
<ul class="simple">
<li><p>Rectified Linear Unit (ReLU)</p></li>
<li><p>Max Pooling</p></li>
<li><p>Dropout</p></li>
<li><p>Batch Normalization</p></li>
<li><p>Skip Connection (Residual Block)</p></li>
<li><p>Adaptive Momentum Estimation (Adam)</p></li>
</ul>
<p>などの技術が、一通り出そろう。</p>
<p>さらには、この頃になるとNVIDIAのCaffeや、モントリオール大学のtheano、FacebookのTorch、そしてPreferred NetworkのChainerといった汎用の深層学習用ライブラリが多数登場する。これによって、深層学習の研究が一気に花開き、現在に至る。</p>
<hr class="docutils" />
<p><strong>Google Colab用の準備</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">IN_COLAB</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">google.colab</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;You are running the code in Google Colab.&quot;</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">IN_COLAB</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;You are running the code in the local computer.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="c1"># PyTorchのインストール</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio

    <span class="c1"># Google Driveのマウント</span>
    <span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
    <span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/drive&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>You are running the code in the local computer.
</pre></div>
</div>
</div>
</div>
<p><strong>下準備のコード</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">glue</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">_</span>

<span class="c1"># 実験に使うサンプル数</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">30000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;epochs&quot;</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;n_samples&quot;</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># グラフの設定</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.dpi&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;colorblind&quot;</span><span class="p">)</span>
<span class="n">color_palette</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;colorblind&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>平仮名73文字データセットの準備</strong></p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">zipfile</span>

<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;http://lab.ndl.go.jp/dataset/hiragana73.zip&quot;</span>
<span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

<span class="c1"># HTTPリクエストを送ってデータサイズを取得</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">total_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content-length&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">65535</span>

<span class="c1"># &quot;hiragana73&quot;フォルダが存在し、その中身が空でないことを確認</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s2">&quot;./hiragana73&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s2">&quot;./hiragana73&quot;</span><span class="p">))</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># 実際のファイルのダウンロード</span>
    <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_size</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="n">unit_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">r</span><span class="o">.</span><span class="n">iter_content</span><span class="p">(</span><span class="n">chunk_size</span><span class="p">):</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">chunk_size</span><span class="p">)</span>

    <span class="c1"># ダウンロードが完了したらZIPを展開する</span>
    <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">extractall</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="pytorch">
<h2><span class="section-number">3.2. </span>PyTorchの基本<a class="headerlink" href="#pytorch" title="Permalink to this heading">#</a></h2>
<p>PyTorchには、いくつかのモジュールが用意されており、代表的なものが、</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span></code></p></li>
</ul>
<p>の3つである。慣例的に、これらをこのような形でエイリアスを与えてインポートする。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorchのモジュール群</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code>モジュールがテンソルデータそのもの(<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>等)や、データに対する操作 (<code class="docutils literal notranslate"><span class="pre">torch.exp</span></code>や<code class="docutils literal notranslate"><span class="pre">torch.transpose</span></code>等)が含まれる。</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>モジュールには、ニューラルネットワークを構成するレイヤー (<code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code>や<code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code>等)や損失関数 (<code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code>や<code class="docutils literal notranslate"><span class="pre">nn.MSELoss</span></code>等)が含まれる。</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span></code>モジュールには、<code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>モジュールに含まれるクラス定義に対応する関数が用意されている。例えば<code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code>に対応する関数として<code class="docutils literal notranslate"><span class="pre">F.linear</span></code>、<code class="docutils literal notranslate"><span class="pre">nn.MSELoss</span></code>に対応する関数として<code class="docutils literal notranslate"><span class="pre">F.mse_loss</span></code>、といった具合である。</p>
<p>また、インストールすることは必須ではないが、PyTorchに付属するライブラリにTorchVisionがある。TorchVisionは、主にコンピュータ・ビジョンへの応用を目的とした補助関数が多数用意されている。</p>
<p>これ以外にも、有名ネットワークモデルの学習済み重みなどが提供されており、<code class="docutils literal notranslate"><span class="pre">AlexNet</span></code>や<code class="docutils literal notranslate"><span class="pre">ResNet50</span></code>等のほか、<code class="docutils literal notranslate"><span class="pre">ViT</span></code>などの比較的新しいものも含まれている。</p>
<p>本資料では、画像の前処理に使う<code class="docutils literal notranslate"><span class="pre">transforms</span></code>モジュールだけを用いる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># TorchVision</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
</pre></div>
</div>
</div>
</div>
<section id="torch-tensor">
<h3><span class="section-number">3.2.1. </span>torch.Tensor<a class="headerlink" href="#torch-tensor" title="Permalink to this heading">#</a></h3>
<p>PyTorchの中で変数を扱う場合、スカラーであってもベクトルであっても、はたまた行列であっても、共通で<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>という型を用いる。これは、NumPyの<code class="docutils literal notranslate"><span class="pre">np.array</span></code>とほとんど同じように使うことができる。</p>
<p>初期化をする方法には、いくつかあるが、大きく分けて、</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>のコンストラクタを呼び出す</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code>関数を用いて<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>を作る</p></li>
<li><p>NumPyの配列を最初に用意して<code class="docutils literal notranslate"><span class="pre">torch.from_numpy</span></code>関数を使う</p></li>
</ul>
<p>の3つの方法がある。順に見ていこう。</p>
<section id="id5">
<h4>torch.Tensorを使う<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h4>
<p>まず、<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>を用いる場合には、通常のPythonやNumPyの配列を指定して初期化する。この時、配列がどのような型であっても、PyTorchの<code class="docutils literal notranslate"><span class="pre">default_dtype</span></code>に指定された型 (初期値は<code class="docutils literal notranslate"><span class="pre">float32</span></code>)の型にキャストされる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_npy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NumPy&#39;s dtype:&quot;</span><span class="p">,</span> <span class="n">x_npy</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x_npy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Torch&#39;s dtype:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NumPy&#39;s dtype: int64
Torch&#39;s dtype: torch.float32
</pre></div>
</div>
</div>
</div>
<p>このように、NumPyの配列としての型は<code class="docutils literal notranslate"><span class="pre">int64</span></code>型であるにも関わらず、<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>を用いることで、型が<code class="docutils literal notranslate"><span class="pre">float32</span></code>に変更されていることが分かる。なお、この初期の型は<code class="docutils literal notranslate"><span class="pre">torch.set_default_dtyoe</span></code>で変更することもできる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">set_default_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x_npy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Torch&#39;s dtype:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># 元に戻しておく</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_default_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Torch&#39;s dtype: torch.float64
</pre></div>
</div>
</div>
</div>
</section>
<section id="id6">
<h4>torch.tensorを使う<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>と関数が似ており、非常に紛らわしいが、厳密に言えば<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>はコンストラクタであり、<code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code>は初期化用のユーティリティ関数である。<code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code>にも、PythonやNumPyの配列を指定して<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>型の多次元配列を作ることができる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_npy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x_npy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Torch&#39;s dtype:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Torch&#39;s dtype: torch.int64
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code>は先ほどとは異なり、PythonやNumPyの配列で定義されている要素の型を引き継ぐ。そのため、上記の例では<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>の<code class="docutils literal notranslate"><span class="pre">dtype</span></code>が<code class="docutils literal notranslate"><span class="pre">int64</span></code>になっている。また、<code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code>関数は、型を指定して<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>を作ることもできる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x_npy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Torch&#39;s dtype:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Torch&#39;s dtype: torch.float32
</pre></div>
</div>
</div>
</div>
<p>また、詳細については後述するが、PyTorchの自動微分の機能を使うために必要な<code class="docutils literal notranslate"><span class="pre">requires_grad</span></code>パラメータを指定することもできる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x_npy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>このように<code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code>関数は<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>のコンストラクタを呼び出す場合と比べて圧倒的に使い勝手が良い。</p>
</section>
<section id="torch-from-numpy">
<h4>torch.from_numpy<a class="headerlink" href="#torch-from-numpy" title="Permalink to this heading">#</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">torch.from_numpy</span></code>関数は<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>と仕様はかなり似ているが、</p>
<ul class="simple">
<li><p>引数としてNumPyの配列しか取ることができない</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dtype</span></code>はNumPyのものを引き継ぐ</p></li>
</ul>
<p>の2点が大きく異なる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_npy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_npy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Torch&#39;s dtype:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Torch&#39;s dtype: torch.int64
</pre></div>
</div>
</div>
</div>
<p>また<code class="docutils literal notranslate"><span class="pre">torch.from_numpy</span></code>は元のNumPyの配列とデータを共有しており、元の配列の値を書き換えるとそれが反映されるという違いがある。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># torch.Tensorの場合</span>
<span class="n">x_npy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x_npy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">x_npy</span> <span class="o">*=</span> <span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; After:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Before: tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
 After: tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># torch.from_numpyの場合</span>
<span class="n">x_npy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_npy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">x_npy</span> <span class="o">*=</span> <span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; After:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Before: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
 After: tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])
</pre></div>
</div>
</div>
</div>
<p>正直なことを言えば、この違いを意識すべき場面は少ないが、時に問題が生じることがあるので、特段の理由がない限りは<code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code>を使うのが、多くの場合で問題を引き起こす危険性が低い。</p>
</section>
</section>
<section id="id7">
<h3><span class="section-number">3.2.2. </span>torch.Tensorからの値の取り出し<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h3>
<p>また<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>型で何らかの演算を行った後で、それをPythonやNumPyの配列に戻したいと思うこともあるだろう。この場合には、Pythonの配列なら<code class="docutils literal notranslate"><span class="pre">tolist</span></code>関数、NumPyの配列なら<code class="docutils literal notranslate"><span class="pre">numpy</span></code>関数を用いる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Pythonの配列に直す</span>
<span class="n">x_list</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type:&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">x_list</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_list</span><span class="p">)</span>

<span class="c1"># NumPyの配列に直す</span>
<span class="n">x_npy</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type:&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">x_npy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_npy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Type: &lt;class &#39;list&#39;&gt;
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Type: &lt;class &#39;numpy.ndarray&#39;&gt;
[0 1 2 3 4 5 6 7 8 9]
</pre></div>
</div>
</div>
</div>
<p>また、配列の値が1つである場合に限り、<code class="docutils literal notranslate"><span class="pre">item</span></code>関数を使って、その1つの値をPythonの数値型として取り出すこともできる。この<code class="docutils literal notranslate"><span class="pre">item</span></code>関数は、要素が2つ以上ある配列に対して呼び出すと例外になるので注意すること。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id8">
<h2><span class="section-number">3.3. </span>自動微分<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h2>
<p>PyTorchに限らず、深層学習を支える重要な技術に<strong>自動微分</strong>がある。関数の微分は、損失関数の最小化といった最適化問題にとって重要な情報であり、例えば、最急降下法やニュートン法といったアルゴリズムは、それぞれ関数の1階微分 (勾配)と、2階微分 (Hesse行列)を用いる。</p>
<p>しかし、このような微分を求めるには、数学的に関数の微分を求めておく必要があり、特に関数が複雑な場合には、それを求めることは困難である (が、従来はMathematicaなどのソフトを使って、勾配を求めるコードを書き出していた)。</p>
<p>また、1階微分であれば、差分法によって近似をすることも可能ではあるが、数値精度の問題は残る。2階微分以上になると差分計算の誤差が蓄積していくため、数値的に満足な結果を得ることは非常に難しくなる。</p>
<section id="id9">
<h3><span class="section-number">3.3.1. </span>自動微分の仕組み<a class="headerlink" href="#id9" title="Permalink to this heading">#</a></h3>
<p><strong>自動微分</strong>は、プログラム的に、とある演算とその微分計算がペアとして定義されており、演算の列によって表される関数の微分は、各演算の微分から、合成関数の微分としての連鎖率 (chain rule)により計算される。</p>
<p>一例として、ここでは、</p>
<div class="math notranslate nohighlight">
\[
f(x) = \cos(x^2)
\]</div>
<p>の微分を例にとって見てみよう。</p>
<p>ここで、<span class="math notranslate nohighlight">\(g(x) = x^2\)</span>, <span class="math notranslate nohighlight">\(h(x) = \cos x\)</span>とすると、<span class="math notranslate nohighlight">\(f\)</span>は<span class="math notranslate nohighlight">\(g\)</span>と<span class="math notranslate nohighlight">\(h\)</span>の合成関数として<span class="math notranslate nohighlight">\(f = h \circ g\)</span>と表せる。言うまでもなく、<span class="math notranslate nohighlight">\(f\)</span>の微分<span class="math notranslate nohighlight">\(f'\)</span>は、<span class="math notranslate nohighlight">\(h\)</span>と<span class="math notranslate nohighlight">\(g\)</span>の微分を用いて、</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f'(x) &amp;= \frac{\text{d}f}{\text{d}x} = \frac{\text{d}h}{\text{d}g} \frac{\text{d}g}{\text{d}x} \\
&amp;= -\sin (g(x)) \cdot 2 x \\
&amp;= -2x \sin (x^2)
\end{aligned}
\end{split}\]</div>
<p>となる。ここで注目してほしいのは、計算の順序が、</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(x\)</span>の値が与えられる</p></li>
<li><p><span class="math notranslate nohighlight">\(y = x^2\)</span>の値を計算する</p></li>
<li><p><span class="math notranslate nohighlight">\(z = \cos(y)\)</span>の値を計算する</p></li>
</ol>
<p>となっているということである。導関数<span class="math notranslate nohighlight">\(f\)</span>の微分を計算するときには、この逆順に、</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(z\)</span>の<span class="math notranslate nohighlight">\(z\)</span>に関する微分として<span class="math notranslate nohighlight">\(\text{d}z/\text{d}z=1\)</span>が与えられる</p></li>
<li><p>既知の<span class="math notranslate nohighlight">\(y\)</span>から<span class="math notranslate nohighlight">\(\text{d}z/\text{d}y = (\text{d}z/\text{d}z) \cdot (\text{d}z/\text{d}y) = 1 \cdot (-\sin(y)) = -\sin(y)\)</span>を計算する</p></li>
<li><p>既知の<span class="math notranslate nohighlight">\(x\)</span>から<span class="math notranslate nohighlight">\(\text{d}z/\text{d}x = (\text{d}z/\text{d}y) \cdot (\text{d}y/ \text{d}x) = (-\sin(y)) \cdot (2 x) = -2x \sin(x^2)\)</span>を計算する</p></li>
</ol>
<p>という流れになっている。従って、各演算<span class="math notranslate nohighlight">\(y=f(x)\)</span>において、</p>
<ul class="simple">
<li><p>演算の入力<span class="math notranslate nohighlight">\(x\)</span>を保持しておく</p></li>
<li><p>演算の導関数<span class="math notranslate nohighlight">\(\text{d}y/\text{d}x\)</span>を定義しておく</p></li>
</ul>
<p>という準備をしておけば、最終出力<span class="math notranslate nohighlight">\(z\)</span>の<span class="math notranslate nohighlight">\(y\)</span>に関する微分<span class="math notranslate nohighlight">\(\text{d}z/\text{d}y\)</span>が与えられれば、連鎖率を用いて<span class="math notranslate nohighlight">\(\text{d}z/\text{d}x\)</span>が求まる、という訳である。</p>
</section>
<section id="id10">
<h3><span class="section-number">3.3.2. </span>自動微分の利用<a class="headerlink" href="#id10" title="Permalink to this heading">#</a></h3>
<p>では、上記の議論をPyTorchを用いて実験してみる。前述の通り、<code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code>関数に<code class="docutils literal notranslate"><span class="pre">requires_grad</span></code>パラメータを指定することで、自動微分により勾配が計算される変数を作ることができる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 変数を作成</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>なお、一度、作成した<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>に対して自動微分を有効にしたい場合には<code class="docutils literal notranslate"><span class="pre">requires_grad_</span></code>関数に<code class="docutils literal notranslate"><span class="pre">True</span></code>を渡す。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>それでは、ここで定義した<span class="math notranslate nohighlight">\(x\)</span>を用いて変数を用いて、<span class="math notranslate nohighlight">\(z = \cos(x^2)\)</span>を段階的に計算してみる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># y = g(x) = x^2の計算</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y = g(x) = </span><span class="si">{:.1f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>

<span class="c1"># z = cos(y)の計算</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;z = h(y) = </span><span class="si">{:.5f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y = g(x) = 4.0
z = h(y) = -0.65364
</pre></div>
</div>
</div>
</div>
<p>最終的な<span class="math notranslate nohighlight">\(z\)</span>の<span class="math notranslate nohighlight">\(x\)</span>に関する微分を求めるには、<code class="docutils literal notranslate"><span class="pre">z.backward()</span></code>という関数を呼び出せば良い。ただし、この関数は単純にはスカラーの出力にしか使えないので注意が必要。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 微分の計算</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>すると、予め自動微分を有効にしておいた変数には<code class="docutils literal notranslate"><span class="pre">grad</span></code>というメンバが追加され、そこに微分の値が代入される。<strong>なお、<code class="docutils literal notranslate"><span class="pre">backward</span></code>関数は、デフォルトでは、一度呼び出すと同じ変数に対して再度呼び出すことはできないようになっている(メモリをできるだけ削減するため)</strong>。同じ関数に対して、何度も<code class="docutils literal notranslate"><span class="pre">backward</span></code>を呼び出したい場合には、<code class="docutils literal notranslate"><span class="pre">backward</span></code>関数のパラメータに<code class="docutils literal notranslate"><span class="pre">retain_graph=True</span></code>を渡すこと (例外のメッセージにも同様のことが書かれている)。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Exception:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Exception: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dzdx_autograd</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;autograd: dz/dx = </span><span class="si">{:.5f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dzdx_autograd</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>

<span class="n">dzdx_analytic</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;analytic: dz/dx = </span><span class="si">{:.5f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dzdx_analytic</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>autograd: dz/dx = 3.02721
analytic: dz/dx = 3.02721
</pre></div>
</div>
</div>
</div>
<p>以上から、自動微分によって、正しく演算の微分が計算できていることが確認できた。</p>
<p>上記と同等の計算は、単に入力となっている<span class="math notranslate nohighlight">\(x\)</span>に関する勾配を求めたいだけであれば <code class="docutils literal notranslate"><span class="pre">torch.autograd.grad</span></code>を用いて、以下のように書くこともできる。なお、<code class="docutils literal notranslate"><span class="pre">torch.autograd.grad</span></code>関数の戻り値は、配列になっているので注意すること。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="n">dzdx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;autograd.grad: dz/dz = </span><span class="si">{:.5f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dzdx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>autograd.grad: dz/dz = 3.02721
</pre></div>
</div>
</div>
</div>
<div class="important admonition">
<p class="admonition-title">計算グラフ</p>
<p>上記の計算では、<span class="math notranslate nohighlight">\(y = x^2\)</span>, <span class="math notranslate nohighlight">\(z = \cos(y)\)</span>として、連鎖律を用いて微分の計算を行った。このように、「ある計算の結果」を「次の計算で用いる」というような、計算の繋がりによって作られるグラフ構造のことを<strong>計算グラフ</strong>と呼ぶ。通常、四則演算や関数の計算などの多くの計算は単項演算 (変数1つに対して行われる演算、<span class="math notranslate nohighlight">\(x^2\)</span>や<span class="math notranslate nohighlight">\(\cos(x)\)</span>など)と二項演算 (変数2つに対して行われる演算、<span class="math notranslate nohighlight">\(x + y\)</span>や<span class="math notranslate nohighlight">\(x^y\)</span>など)に分けられ、3つ以上の変数が絡む演算も基本的には単項演算と二項演算の組み合わせによって表現できる。</p>
<p>自動微分においては、計算の過程でこのような計算グラフをライブラリが内部的に構築しており、グラフを遡っていくことで、「最終的な出力」の「グラフ中に現れた変数」に関する微分を計算している。PyTorchの<code class="docutils literal notranslate"><span class="pre">backward</span></code>等の関数に渡せるパラメータの中にも<code class="docutils literal notranslate"><span class="pre">retain_graph</span></code>や<code class="docutils literal notranslate"><span class="pre">create_graph</span></code>など、「グラフ」という言葉を含むものがあるのはこのためである。</p>
</div>
</section>
<section id="id11">
<h3><span class="section-number">3.3.3. </span>自動微分可能な演算の定義<a class="headerlink" href="#id11" title="Permalink to this heading">#</a></h3>
<p>PyTorchを使うと、自分で微分可能な演算を定義することもできる。関数を定義するための一般的な方法は、<code class="docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code>を継承したクラスを定義し、そこに静的メソッドとして<code class="docutils literal notranslate"><span class="pre">forward</span></code>と<code class="docutils literal notranslate"><span class="pre">backward</span></code>の二つの関数を実装するというものである。</p>
<p><code class="docutils literal notranslate"><span class="pre">forward</span></code>内で計算済みの変数で、<code class="docutils literal notranslate"><span class="pre">backward</span></code>の計算でも使うものは<code class="docutils literal notranslate"><span class="pre">ctx.save_for_backward(...)</span></code>を用いて<code class="docutils literal notranslate"><span class="pre">backward</span></code>関数に渡すことができる。変数の取り出しには<code class="docutils literal notranslate"><span class="pre">ctx.saved_tensors</span></code>を用いる。以下の例では、<span class="math notranslate nohighlight">\(\cos(x)\)</span>を例にとって、実際に微分可能な演算を定義してみる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Function</span>


<span class="k">class</span> <span class="nc">MyCosine</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="k">return</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>この実装では、<code class="docutils literal notranslate"><span class="pre">forward</span></code>の中で、<span class="math notranslate nohighlight">\(y = \cos(x)\)</span>として、戻り値を計算した後に、入力の<span class="math notranslate nohighlight">\(x\)</span>と出力の<span class="math notranslate nohighlight">\(y\)</span>の値を<code class="docutils literal notranslate"><span class="pre">save_for_backward(x,</span> <span class="pre">y)</span></code>として<code class="docutils literal notranslate"><span class="pre">backward</span></code>側でも使えるようにしている。今回の計算では、<span class="math notranslate nohighlight">\(\cos(x)\)</span>の微分が<span class="math notranslate nohighlight">\(-\sin(x)\)</span>であるため、必ずしも<span class="math notranslate nohighlight">\(y\)</span>を<code class="docutils literal notranslate"><span class="pre">backward</span></code>側で使えるようにしておく必要はない。しかし、例えば<span class="math notranslate nohighlight">\(\exp(x)\)</span>やシグモイド関数<span class="math notranslate nohighlight">\(1 / (1 + \exp(x))\)</span>のように、導関数のなかに自分自身を含むようなものも多く、計算量の観点から、<code class="docutils literal notranslate"><span class="pre">forward</span></code>での出力を<code class="docutils literal notranslate"><span class="pre">backward</span></code>側で使えるようにしておくことが多い。</p>
<p>この<code class="docutils literal notranslate"><span class="pre">Function</span></code>型のサブクラスは<code class="docutils literal notranslate"><span class="pre">MyCosine.apply</span></code>のように呼び出すことで関数の<code class="docutils literal notranslate"><span class="pre">forward</span></code>が呼び出されて、その計算結果が使われた出力において<code class="docutils literal notranslate"><span class="pre">backward</span></code>が呼び出されると、自動的に<code class="docutils literal notranslate"><span class="pre">MyCosine</span></code>の<code class="docutils literal notranslate"><span class="pre">backward</span></code>のその計算の中で呼び出されるようになる。</p>
<p>PyTorch内部の実装においては、上記のような<code class="docutils literal notranslate"><span class="pre">Function</span></code>のサブクラスを内部で呼び出すような関数を定義している場合が多く、それに従って<code class="docutils literal notranslate"><span class="pre">my_cos</span></code>関数を定義しておく。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_cos</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">MyCosine</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>これを用いて、再度 <span class="math notranslate nohighlight">\(\cos(x^2)\)</span>の微分を計算してみると、以下のように正しく計算が行えていることが分かる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">my_cos</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;my cosine: dzdx = </span><span class="si">{:.5f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>my cosine: dzdx = 3.02721
</pre></div>
</div>
</div>
</div>
</section>
<section id="id12">
<h3><span class="section-number">3.3.4. </span>二階微分の計算<a class="headerlink" href="#id12" title="Permalink to this heading">#</a></h3>
<p>続いて、前述の<span class="math notranslate nohighlight">\(f(x) = \cos(x^2)\)</span>の二階微分<span class="math notranslate nohighlight">\(f^{''}(x) = -2 \sin(x^2) - 4x^2 \cos(x^2)\)</span>の計算を自動微分で行ってみる。</p>
<p>実は、自動微分を使えば、高階微分を計算することも容易で、二階導関数を求めたい場合、一階導関数の計算中に計算グラフを遡っていく計算に対して、別の計算グラフを構築すれば良い。この計算グラフを再度遡って微分を求めることで二階微分が求まる、というわけである。</p>
<p>例えば、先ほどの計算であれば、連鎖律の途中で、<span class="math notranslate nohighlight">\(2x\)</span>の計算や<span class="math notranslate nohighlight">\(-\sin(x)\)</span>の計算が発生していたが、これらの計算について、途中結果を保存し、その微分計算が行えるように計算グラフを構築することができる。なお、二階微分を計算するときには<code class="docutils literal notranslate"><span class="pre">backward</span></code>関数の代わりに、<code class="docutils literal notranslate"><span class="pre">torch.autograd.grad</span></code>を使わないと警告メッセージが出るので注意すること (計算自体はできる)。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">dzdx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ddz_ddx_auto</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">dzdx</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="n">ddz_ddx_analy</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">-</span> <span class="mf">4.0</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;autograd: ddz_ddx = </span><span class="si">{:.5f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ddz_ddx_auto</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;analytic: ddz_ddx = </span><span class="si">{:.5f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ddz_ddx_analy</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>autograd: ddz_ddx = 11.97190
analytic: ddz_ddx = 11.97190
</pre></div>
</div>
</div>
</div>
<p>このように、二階微分の場合も正しく計算できていることが分かる。以後、より高階な微分であっても<code class="docutils literal notranslate"><span class="pre">torch.autograd.grad</span></code>の引数で<code class="docutils literal notranslate"><span class="pre">create_graph=True</span></code>を指定する限りは計算し続けることができる。</p>
</section>
<section id="id13">
<h3><span class="section-number">3.3.5. </span>多変数関数の微分<a class="headerlink" href="#id13" title="Permalink to this heading">#</a></h3>
<p>続いては、変数が2つ以上の場合の微分 (勾配)について見てみる。今回は例としてReosenbrock関数と呼ばれる、以下の関数について微分を計算してみる。</p>
<div class="math notranslate nohighlight">
\[
f(x, y) = a (x - 1)^2 + b(y - x^2)^2
\]</div>
<p>この関数において<span class="math notranslate nohighlight">\(a = 1\)</span>, <span class="math notranslate nohighlight">\(b = 100\)</span>とするとして、二次元平面上に値をプロットすると以下のようになる (カラーバーは対数の値に対して計算されている)。</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">LogNorm</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">extent</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
<span class="n">rosen</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">xs</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">ys</span> <span class="o">-</span> <span class="n">xs</span><span class="o">**</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">mappable</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">rosen</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="n">extent</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">LogNorm</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.0e4</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Rosenbrock function&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">rosen</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;white&quot;</span><span class="p">],</span> <span class="n">linewidths</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">norm</span><span class="o">=</span><span class="n">LogNorm</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.0e4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color_palette</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">mappable</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/4e40c37e814a907228152bb6d80163405f7e52ad3c066e45a5754bd2f7ce6a85.png" src="../../_images/4e40c37e814a907228152bb6d80163405f7e52ad3c066e45a5754bd2f7ce6a85.png" />
</div>
</div>
<p>この関数は、<span class="math notranslate nohighlight">\((1.0, 1.0)\)</span>の点を打った場所が関数の最小値をとる箇所になっているのだが、最小値の近傍が非常に狭い谷のような形になっており、さらにその谷が放物線上に湾曲しているため、この赤点の位置の最小値を求めることが困難であるとされている。</p>
<p>まずは<span class="math notranslate nohighlight">\((x, y) = (0, 0)\)</span>として、Rosenbrock関数自体の値を計算してみる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1., grad_fn=&lt;AddBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>次に、単一の変数の場合と同様に、出力の<code class="docutils literal notranslate"><span class="pre">f</span></code>に対して<code class="docutils literal notranslate"><span class="pre">backward</span></code>を呼び出して、<span class="math notranslate nohighlight">\(x, y\)</span> (上記のコードでは<code class="docutils literal notranslate"><span class="pre">x</span></code>の0番目と1番目の要素に対応)に関する微分を求める。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-2.,  0.])
</pre></div>
</div>
</div>
</div>
<p>Rosenbrock関数の<span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(y\)</span>に関する偏微分は、それぞれ</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\frac{\partial f}{\partial x} &amp;= 2a (x -1) - 4bx(y - x^2) \\
\frac{\partial f}{\partial y} &amp;= 2b (y - x^2)
\end{align}
\end{split}\]</div>
<p>であるので、<span class="math notranslate nohighlight">\(a = 1, b= 100\)</span>かつ<span class="math notranslate nohighlight">\((x, y) = (0, 0)\)</span>であるとき、導関数の値は<span class="math notranslate nohighlight">\((-2, 0)\)</span>になっており、上記の自動微分による結果と一致する。</p>
<hr class="docutils" />
<p>続いては、Rosenbrock関数の二階導関数としてのHesse行列 (Hessianとも言う)を求めてみる。この場合は、先ほどの1変数の場合よりは多少工夫が必要になる。
まずは、<code class="docutils literal notranslate"><span class="pre">f</span></code>に対して<code class="docutils literal notranslate"><span class="pre">torch.autograd.grad</span></code>を計算する。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([-2.,  0.], grad_fn=&lt;AddBackward0&gt;),)
</pre></div>
</div>
</div>
</div>
<p>安直には、この<code class="docutils literal notranslate"><span class="pre">grad</span></code>に対して、もう一度<code class="docutils literal notranslate"><span class="pre">torch.autograd.grad</span></code>関数を適用すれば良さそうだが、前述の通り<code class="docutils literal notranslate"><span class="pre">backward</span></code>関数や<code class="docutils literal notranslate"><span class="pre">torch.autograd.grad</span></code>関数は、出力がスカラー出ない場合には使うことができない。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Exception:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Exception: grad can be implicitly created only for scalar outputs
</pre></div>
</div>
</div>
</div>
<p>従って、ここで計算に一工夫が必要になる。ここで連鎖律の計算を思い出してほしい。連鎖律を計算するとき、スカラー値スカラー関数の場合には、<span class="math notranslate nohighlight">\(\text{d}z / \text{d}z = 1\)</span>から、連鎖律が始まり、その前の計算の微分を順に乗していくことで最終的な入力変数に関する微分を計算していたのであった。</p>
<p>この理屈で言えば、出力が二次元ベクトルであるような関数において、連鎖律のスタートとなるべき値は</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\left( \frac{\partial x}{\partial x}, \frac{\partial y}{\partial x} \right) = (1, 0)
\left( \frac{\partial x}{\partial y}, \frac{\partial y}{\partial y} \right) = (0, 1)
\end{align}
\]</div>
<p>の2つとなることに気づく。そこで、このそれぞれを連鎖律のスタートとして<code class="docutils literal notranslate"><span class="pre">grad_outputs</span></code>パラメータに指定して<code class="docutils literal notranslate"><span class="pre">torch.autograd.grad</span></code>を呼び出してみる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ddf_dxx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
    <span class="n">grad</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ddf_dyy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
    <span class="n">grad</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">ddf_dxx</span><span class="p">,</span> <span class="n">ddf_dyy</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[  2.,   0.],
        [  0., 200.]])
</pre></div>
</div>
</div>
</div>
<p>Rosenbrock関数のHesse行列は、解析的には</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H} = \begin{bmatrix}
2a + 4b (y - x^2) + 8bx^2 &amp; -4bx \\
-4bx &amp; 2b
\end{bmatrix}
\end{split}\]</div>
<p>であるので、<span class="math notranslate nohighlight">\(a = 1, b = 100\)</span>, <span class="math notranslate nohighlight">\((x, y) = (0, 0)\)</span>の時には、</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H} = \begin{bmatrix}
2 &amp; 0 \\ 0 &amp; 200
\end{bmatrix}
\end{split}\]</div>
<p>となり、自動微分の結果が解析的な微分結果と一致していることが分かる。</p>
</section>
</section>
<section id="id14">
<h2><span class="section-number">3.4. </span>ニュートン法の実装<a class="headerlink" href="#id14" title="Permalink to this heading">#</a></h2>
<p>それでは、ここで練習としてニュートン法を用いてRosenbrock関数の最小値を求めてみよう。ニュートン法は、Hesse行列<span class="math notranslate nohighlight">\(\mathbf{H}\)</span>と、<span class="math notranslate nohighlight">\(f\)</span>の勾配<span class="math notranslate nohighlight">\(\nabla f\)</span>を用いて、</p>
<div class="math notranslate nohighlight" id="equation-eq-newton-step">
<span class="eqno">(3.1)<a class="headerlink" href="#equation-eq-newton-step" title="この数式へのパーマリンク">#</a></span>\[
\boldsymbol\delta = \mathbf{H}^{-1} \nabla f
\]</div>
<p>のように更新幅を計算するような、繰り返し最適化法の一種である。これは、関数<span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span>の<span class="math notranslate nohighlight">\(\boldsymbol\delta\)</span>周りのTaylor展開により、</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x} + \boldsymbol\delta) \approx f(\mathbf{x}) + \frac{1}{1!} \boldsymbol\delta^\top \nabla f(\mathbf{x}) + \frac{1}{2!} \boldsymbol\delta^\top \mathbf{H} \boldsymbol\delta
\]</div>
<p>となることに起因する。この式を変形すると、</p>
<div class="math notranslate nohighlight">
\[
\frac{\text{d} f}{\text{d} \boldsymbol\delta}(\mathbf{x}) = \nabla f(\mathbf{x}) + \frac{1}{2} \mathbf{H}\boldsymbol\delta
\]</div>
<p>という式が得られる。従って、Taylor展開の第2項までで元の関数を近似した範囲においては、<span class="math notranslate nohighlight">\(\text{d} f / \text{d}\boldsymbol\delta = \mathbf{0}\)</span>となるような場所に移動することで、関数の最小値に近づくことができる (これは、関数を局所的に二次関数で近似して、その二次関数の「底」に移動することに対応する)。</p>
<p>実際には、最小化すべき関数が局所的に二次関数で近似できることばかりではないので、通常は<a class="reference internal" href="#equation-eq-newton-step">(3.1)</a>で求まった更新方向に小さな定数<span class="math notranslate nohighlight">\(\alpha\)</span>を乗じて<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>の値を</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{t+1} = \mathbf{x}^t + \alpha \boldsymbol\delta
\]</div>
<p>のように更新することが多い。</p>
<p>では、ここまでの議論を踏まえて、実際に自動微分により求めたHesse行列を用いてRosenbrock関数を最小化してみよう (以下にコードと実行結果を示すが、まずは自分自身で考えてみてほしい)。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Rosenbrock function&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">initial_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">calc_newton_step</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">gx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
        <span class="n">grad</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">gy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
        <span class="n">grad</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>


<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">pts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">fx</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">calc_newton_step</span><span class="p">(</span><span class="n">fx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">dx</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The answer is:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The answer is: [1. 1.]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">LogNorm</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">extent</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
<span class="n">rosen</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">xs</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">ys</span> <span class="o">-</span> <span class="n">xs</span><span class="o">**</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">mappable</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">rosen</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="n">extent</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">LogNorm</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.0e4</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Rosenbrock function&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">rosen</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;white&quot;</span><span class="p">],</span> <span class="n">linewidths</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">norm</span><span class="o">=</span><span class="n">LogNorm</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.0e4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color_palette</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">initial_x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">initial_x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color_palette</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># 軌跡のプロット</span>
<span class="n">pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pts</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pts</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">pts</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color_palette</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="s2">&quot;3.0&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">mappable</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/1582c9d0eb2246c11d36b2e41f13556d807ae648cb32649cbd51887a8a87e562.png" src="../../_images/1582c9d0eb2246c11d36b2e41f13556d807ae648cb32649cbd51887a8a87e562.png" />
</div>
</div>
<p>この図では、<span class="math notranslate nohighlight">\((0.0, 0.0)\)</span>の初期値から<span class="math notranslate nohighlight">\((1.0, 1.0)\)</span>の最小値に至るまでの最適化の過程をマーカー付きの曲線で示している。各マーカーの位置を見てみると、徐々に最小値に至るスピードが遅くなりつつも、正しく関数の最小値を取る箇所に収束していることが分かる。</p>
<p>Rosenbrock関数の最小化については、解の初期値やニュートン法のステップ幅を変化させることで、収束が不安定になって最小解からはずれて・近づいてを繰り返すような軌跡を描くこともある。ぜひ、いろいろなパラメータで軌跡を描画して、その性質の理解に努めて欲しい。</p>
<div class="admonition note">
<p class="admonition-title">注釈</p>
<p>ここでは、関数の勾配を求めて、勾配法により関数を最小化する問題を解いた。深層学習は、ニューラルネットのパラメータを変数として、その変数を同じく勾配法により最適化する問題であり、最小化する関数は<strong>損失関数</strong>(loss function)と呼ばれる (反対に目的を達成するために最大化される関数を<strong>目的関数</strong>と呼ぶ)。</p>
<p>ニューラルネットのパラメータ最適化(=訓練)には、ニュートン法や準ニュートン法のような損失関数の二階微分を考慮するような方法を用いることは少なく (ただしAdaSecant<span id="id15">[<a class="reference internal" href="#id62" title="Caglar Gulcehre, Marcin Moczulski, and Yoshua Bengio. Adasecant: robust adaptive secant method for stochastic gradient. In IEEE International Joint Conference on Neural Network. 2014.">Gulcehre <em>et al.</em>, 2014</a>]</span>のような二階微分を考慮する方法もある)、多くの場合は単純な確率的最急降下法やRMSprop, Adamのようなアルゴリズムが使われることが多い。これは、パラメータ数が多くなると、Hesse行列を求めるのに多くの計算量が必要になるためで、そうであれば、一階微分だけが求まれば実行できる最急降下法を安定化させるように工夫する方が良い、という発想である。</p>
</div>
<section id="id16">
<h3><span class="section-number">3.4.1. </span>オプティマイザを利用した最適化<a class="headerlink" href="#id16" title="Permalink to this heading">#</a></h3>
<p>PyTorchには<strong>オプティマイザ</strong>というモジュールが用意されており、一階微分量を用いる最急降下法を対象として、さまざまなアルゴリズムが提供されている。ニューラルネットワークの訓練においては、確率的に選ばれたミニバッチから、パラメータを更新する勾配を求めるので、モジュールの名前としては確率的最急降下法 (SGD = stochastic gradient descent)を基本としたものとなっている。</p>
<section id="id17">
<h4>確率的最急降下法<a class="headerlink" href="#id17" title="Permalink to this heading">#</a></h4>
<p>最も単純な<strong>確率的最急降下法</strong>では、誤差関数を<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>、パラメータを<span class="math notranslate nohighlight">\(\theta\)</span>、更新のステップ幅 (=更新率)を<span class="math notranslate nohighlight">\(\gamma\)</span>として、パラメータ<span class="math notranslate nohighlight">\(\theta\)</span>を以下の式で更新する。</p>
<div class="math notranslate nohighlight">
\[
\theta_{t+1} = \theta_{t} - \gamma \frac{\partial\mathcal{L}}{\partial \theta_{t}}
\]</div>
<p>しかし、単純な確率的最急降下法はパラメータの更新方向が安定しないという問題があり、<strong>モメンタム</strong>(慣性)を使って、急激に勾配方向が変わらないように移動平均を取るアルゴリズム (<strong>Momentum SGD</strong>)もある。Momentum SGDの更新式は以下の通り (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">参照</a>)。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\theta_{t+1} &amp;= \theta_{t} - \gamma g_{t+1} \\
g_{t+1} &amp;= \mu g_{t} + \frac{\partial\mathcal{L}}{\partial \theta_{t}}
\end{align}
\end{split}\]</div>
<p>このように、慣性パラメータとして<span class="math notranslate nohighlight">\(\mu\)</span> (<span class="math notranslate nohighlight">\(0 &lt; \mu &lt; 1\)</span>)を導入することで、過去の更新方向を一定の割合で残しつつ、パラメータの更新を行う。</p>
</section>
<section id="rmsprop">
<h4>RMSprop<a class="headerlink" href="#rmsprop" title="Permalink to this heading">#</a></h4>
<p>最急降下法のもう一つの問題に<strong>解の振動</strong>が挙げられる。これはパラメータ更新のステップ幅が大きすぎるために、谷のような形状の両岸を行ったり来たりしてしまうような現象である。当然、ステップサイズを小さくすればその影響は抑えられるが、その分、最適化の収束は遅くなってしまう。</p>
<p>このような振動の問題を防ぐアルゴリズムの一つに<strong>RMSprop</strong> (root-mean-square propagation)がある。RMSpropは、その名前にある通り、勾配の二乗平均平方根を取り、その値が履歴として大きいパラメータの更新を抑制する。振動が起こっている時は、本来更新しなくても良い方向に行ったり来たりしてしまっているわけだから、このような過去の更新量に基づく調整が有効に働くことが分かるだろう。</p>
<p>RMSpropのパラメータ更新式は以下の通り (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html">参考</a>)。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\theta_{t+1} &amp;= \theta_t + \gamma \frac{g_{t+1}}{\sqrt{v_{t+1} + \epsilon}} \\
v_{t+1} &amp;= \alpha v_{t} + (1 - \alpha) g_{t+1}^2 \\
g_{t+1} &amp;= \frac{\partial\mathcal{L}}{\partial\theta_{t}}
\end{align}
\end{split}\]</div>
<p>この式が示すとおり<span class="math notranslate nohighlight">\(v_{t}\)</span>は、過去の勾配の二乗を時間平均したもので、その平方根の逆数を勾配<span class="math notranslate nohighlight">\(g_t\)</span>に乗ずることで、過去に多く更新されているパラメータの更新を抑制している。なお、<span class="math notranslate nohighlight">\(\epsilon\)</span>はゼロ除算を防ぐための定数でPyTorchでは初期値として<span class="math notranslate nohighlight">\(1 \times 10^{-8}\)</span>が設定されている。</p>
</section>
<section id="adam">
<h4>Adam<a class="headerlink" href="#adam" title="Permalink to this heading">#</a></h4>
<p><strong>Adam</strong> (adaptive momentum method)は、前述のMomentum SGDとRMSpropを組み合わせたアルゴリズムで、確率的最急降下法の勾配方向の不安定性と振動の問題を両方解決するように設計されている。</p>
<p>Adamには更新率<span class="math notranslate nohighlight">\(\gamma\)</span>と合わせて、二つのパラメータ<span class="math notranslate nohighlight">\(\beta_1\)</span>と<span class="math notranslate nohighlight">\(\beta_2\)</span>を設定する。これらのうち<span class="math notranslate nohighlight">\(\beta_1\)</span>は、勾配方向に慣性を調整するパラメータで1に近い値を取るほど、強く慣性が働き、過去の勾配の影響を強く残す。一方、<span class="math notranslate nohighlight">\(\beta_2\)</span>は、振動の抑制に働くパラメータで、1に近い値が取るほど、過去の勾配の大きさを考慮して更新量を抑制するようになる。</p>
<p>また、これに加えて、<span class="math notranslate nohighlight">\(\beta_1\)</span>, <span class="math notranslate nohighlight">\(\beta_2\)</span>を用いて、最適化の始めは大きなステップサイズで更新を行い、徐々にその効果を弱めていくという計算もなされている。</p>
</section>
<section id="id18">
<h4>オプティマイザを使用した最適化<a class="headerlink" href="#id18" title="Permalink to this heading">#</a></h4>
<p>深層学習では、上記のSGDやAdam等のオプティマイザを用いてニューラルネットワークのパラメータを最適化するのだが、この仕組みは最急降下法等の一階微分を用いる最適化問題にも使用することができる。</p>
<p>そこで、深層学習に進む前に、まずは前述のRosenbrock関数を上記のオプティマイザを使って最適化し、その違いについて見てみよう。</p>
<p>データローダとは、PyTorchを用いたニューラルネットワークの学習において、ミニバッチ学習を簡単にするための仕組みである。通常、深層学習には大量の訓練データが必要であり、それら全てを考慮したパラメータの更新方向(=勾配)を求めることは現実的ではない。</p>
<p>そこで、大量の訓練データから少数のデータ、すなわちミニバッチをサンプルし、そのミニバッチ内のデータによって与えられる勾配が、データ全体から求まる勾配の近似として十分に正しく動作することを仮定する。データから収集してくるミニバッチの数は<code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>型のサブクラスとして用意されたデータセット・クラスを引数にとる<code class="docutils literal notranslate"><span class="pre">torch.data.utils.data.DataLoader</span></code>によって制御できる。</p>
<p>では、上記のひらがな73文字データセットについて、まずはデータの読み出しを行う役割を持つデータセット・クラスを作成してみよう。データセット・クラスは<code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>型のサブクラスとして実装する。この際、コンストラクタと合わせて、データの総数を返す<code class="docutils literal notranslate"><span class="pre">__len__</span></code>関数と、データ1つをサンプルする<code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>関数の二つを実装する。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optims</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;SGD&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">x</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2.0e-3</span><span class="p">),</span>
    <span class="s2">&quot;Momentum-SGD&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">x</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2.0e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">),</span>
    <span class="s2">&quot;RMSprop&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">([</span><span class="n">x</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2.0e-3</span><span class="p">),</span>
    <span class="s2">&quot;Adam&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">x</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2.0e-3</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">plots</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">opt</span> <span class="ow">in</span> <span class="n">optims</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">opt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
        <span class="c1"># 現在の点を保存</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

        <span class="c1"># パラメータの更新</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">plots</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">extent</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
<span class="n">rosen</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">xs</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">ys</span> <span class="o">-</span> <span class="n">xs</span><span class="o">**</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">pts</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">plots</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">mappable</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">rosen</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
        <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span>
        <span class="n">extent</span><span class="o">=</span><span class="n">extent</span><span class="p">,</span>
        <span class="n">norm</span><span class="o">=</span><span class="n">LogNorm</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.0e4</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span>
        <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">rosen</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;white&quot;</span><span class="p">],</span> <span class="n">linewidths</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">norm</span><span class="o">=</span><span class="n">LogNorm</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.0e4</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">initial_x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">initial_x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color_palette</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">100</span>
    <span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color_palette</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">pts</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">pts</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="c1"># marker=&quot;o&quot;,</span>
        <span class="n">markersize</span><span class="o">=</span><span class="s2">&quot;2.0&quot;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">color_palette</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/89da12c2dad04ea5ba02a419926b61d4ba6042c1e933b67d765f496b558c9157.png" src="../../_images/89da12c2dad04ea5ba02a419926b61d4ba6042c1e933b67d765f496b558c9157.png" />
</div>
</div>
<p>上記の結果を見てみると、SGDは最初の方で、解が大きく振動しているのに対して、Momentum SGDでは、それが多少緩和されていることが分かる。また、更新量から、その勾配を調整する仕組みが入っているRMSpropやAdamでは、ほとんど解が振動することなく、Rosenbrock関数の谷に沿って、解が収束している。</p>
<p>このような、最適化手法の性質の違いに留意しつつ、適切なオプティマイザを選ぶことが好ましい。ただし、オプティマイザに関しては、Adamの発展形などもいろいろと提案されており、新しいものを使おうとすると切りがないため、深層学習を使って研究する場合には、現在、他の多くの研究で用いられているものを使っておくのが無難だろう。</p>
</section>
</section>
</section>
<section id="id19">
<h2><span class="section-number">3.5. </span>深層学習<a class="headerlink" href="#id19" title="Permalink to this heading">#</a></h2>
<p>PyTorchを使った深層学習をするために準備すべきことはいくつかある。以下では、</p>
<ul class="simple">
<li><p><a class="reference internal" href="#ssec-dataloader-preparation"><span class="std std-ref">データローダの準備</span></a></p></li>
<li><p><a class="reference internal" href="#ssec-network-architecture"><span class="std std-ref">ネットワークの構築</span></a></p></li>
<li><p><a class="reference internal" href="#ssec-optimizer-preparation"><span class="std std-ref">オプティマイザの準備</span></a></p></li>
<li><p><a class="reference internal" href="#ssec-loss-function"><span class="std std-ref">損失関数の設定</span></a></p></li>
<li><p><a class="reference internal" href="#ssec-training-loop"><span class="std std-ref">学習ループの実装</span></a></p></li>
</ul>
<p>のそれぞれについて順に説明する。</p>
<section id="ssec-dataloader-preparation">
<span id="id20"></span><h3><span class="section-number">3.5.1. </span>データローダの作成<a class="headerlink" href="#ssec-dataloader-preparation" title="Permalink to this heading">#</a></h3>
<p>データローダとは、PyTorchを用いたニューラルネットワークの学習において、ミニバッチ学習を簡単にするための仕組みである。通常、深層学習には大量の訓練データが必要であり、それら全てを考慮したパラメータの更新方向(=勾配)を求めることは現実的ではない。</p>
<p>そこで、大量の訓練データから少数のデータ、すなわちミニバッチをサンプルし、そのミニバッチ内のデータによって与えられる勾配が、データ全体から求まる勾配の近似として十分に正しく動作することを仮定する。データから収集してくるミニバッチの数は<code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>型のサブクラスとして用意されたデータセット・クラスを引数にとる<code class="docutils literal notranslate"><span class="pre">torch.data.utils.data.DataLoader</span></code>によって制御できる。</p>
<p>では、上記のひらがな73文字データセットについて、まずはデータの読み出しを行う役割を持つデータセット・クラスを作成してみよう。データセット・クラスは<code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>型のサブクラスとして実装する。この際、コンストラクタと合わせて、データの総数を返す<code class="docutils literal notranslate"><span class="pre">__len__</span></code>関数と、データ1つをサンプルする<code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>関数の二つを実装する。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>


<span class="k">class</span> <span class="nc">HiraganaDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ひらがな73文字データセット</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataroot</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HiraganaDataset</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dataroot</span> <span class="o">=</span> <span class="n">dataroot</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>

        <span class="c1"># 各ひらがなの画像が入っているフォルダを列挙</span>
        <span class="n">folders</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataroot</span><span class="p">)])</span>
        <span class="n">chars</span> <span class="o">=</span> <span class="p">[</span><span class="nb">chr</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;U&quot;</span><span class="p">,</span> <span class="s2">&quot;0x&quot;</span><span class="p">),</span> <span class="mi">16</span><span class="p">))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">folders</span><span class="p">]</span>
        <span class="n">n_chars</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
        <span class="n">char2num</span> <span class="o">=</span> <span class="p">{</span><span class="n">c</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
        <span class="n">folders</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataroot</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">folders</span><span class="p">]</span>

        <span class="c1"># 各フォルダに含まれる画像ファイルを列挙、配列に格納</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">folders</span><span class="p">:</span>
            <span class="n">char</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;U&quot;</span><span class="p">,</span> <span class="s2">&quot;0x&quot;</span><span class="p">)</span>
            <span class="n">char</span> <span class="o">=</span> <span class="nb">chr</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">char</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
            <span class="n">num</span> <span class="o">=</span> <span class="n">char2num</span><span class="p">[</span><span class="n">char</span><span class="p">]</span>

            <span class="n">image_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
            <span class="n">image_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">image_files</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.png&quot;</span><span class="p">)]</span>
            <span class="n">image_files</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">image_files</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">extend</span><span class="p">([(</span><span class="n">f</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">image_files</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ファイルの総数を返す&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;データ1つをサンプルする&quot;&quot;&quot;</span>
        <span class="n">image_file</span><span class="p">,</span> <span class="n">num</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_file</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">image</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">IOError</span><span class="p">(</span><span class="s2">&quot;Failed to load image: </span><span class="si">{:s}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">num</span>
</pre></div>
</div>
</div>
</div>
<p>さて、上記のデータセット・クラスにはコンストラクタの引数に<code class="docutils literal notranslate"><span class="pre">transform</span></code>という変数が渡されている。PyTorchではTorchVisionの<code class="docutils literal notranslate"><span class="pre">transforms</span></code>モジュールに用意されたデータ操作のためのクラスを用いることで、簡単にデータの前処理を行うことができる。</p>
<p>例えば、<a class="reference internal" href="feature-extraction.html#sec-feature-extraction"><span class="std std-ref">特徴量の抽出</span></a>で行っていたような</p>
<ul class="simple">
<li><p>画像をグレースケールに変更</p></li>
<li><p>画像をランダムに回転、拡大・縮小</p></li>
</ul>
<p>といった操作は <code class="docutils literal notranslate"><span class="pre">transforms.GrayScale</span></code>や<code class="docutils literal notranslate"><span class="pre">transforms.RandomAffine</span></code>によって実現することができる。複数の前処理操作を組み合わせる場合には、<code class="docutils literal notranslate"><span class="pre">transforms.Compose</span></code>に前処理を行うクラス・インスタンスの配列を渡せば良い。</p>
<p>PyTorchの学習には、<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>型かつ<code class="docutils literal notranslate"><span class="pre">float32</span></code>型の変数を用いるので、上記の二つの前処理と合わせて、型の変換を行う<code class="docutils literal notranslate"><span class="pre">transforms.ToTensor</span></code> (<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>型への変更)と<code class="docutils literal notranslate"><span class="pre">transforms.ConvertImageDtype</span></code> (データ内部の数値型を指定された型に変更する)を<code class="docutils literal notranslate"><span class="pre">transforms.Compose</span></code>に与えている。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Grayscale</span><span class="p">(),</span>  <span class="c1"># 画像のグレースケール化</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomAffine</span><span class="p">(</span><span class="n">degrees</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">60</span><span class="p">,</span> <span class="mi">60</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]),</span>  <span class="c1"># ランダム回転、拡大・縮小</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>  <span class="c1"># torch.Tensorに型変換</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ConvertImageDtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>  <span class="c1"># データ型を32bit浮動小数に変換</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">transform</span></code>の準備ができたら、これを前処理計算として、データセット・クラスをインスタンス化する。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">HiraganaDataset</span><span class="p">(</span><span class="n">dataroot</span><span class="o">=</span><span class="s2">&quot;hiragana73&quot;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>このようにして作られたデータセットクラスはscikit-learnの時と同様に <code class="docutils literal notranslate"><span class="pre">torch.utils.data.random_split</span></code>関数を使うことで、訓練用とテスト用にデータを分割することができる。以下では、<span class="output text_plain">30000</span>個を訓練データとして、10000個をテスト用データとして用いている。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">n_samples</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">n_samples</span><span class="p">])</span>
<span class="n">test_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">n_samples</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">n_samples</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>データセットの分割が完了したら、最後に<code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>のインスタンス化を行う。このクラスはデータに対するイテレータとして用いることができ、予め<code class="docutils literal notranslate"><span class="pre">batch_size=...</span></code>で指定した数のデータを含むミニバッチを順に取り出してくれる。この際、データの順序をランダムにシャッフルするかどうかは<code class="docutils literal notranslate"><span class="pre">shuffle=...</span></code>で制御できる。訓練データはシャッフルを行い、テストデータはシャッフルを行わないで用いることが多い。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="ssec-network-architecture">
<span id="id21"></span><h3><span class="section-number">3.5.2. </span>ネットワークの構築<a class="headerlink" href="#ssec-network-architecture" title="Permalink to this heading">#</a></h3>
<p>学習可能なニューラルネットワークは<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>クラスを継承することで作成できる。まずは、単純な実装として、ひらがなの画像 (48×48画素)を2304次元ベクトルとして扱う場合について見てみる。</p>
<p>今回は、ひらがなの種類が73次元であるので、ネットワークが出力するべきものは73次元のベクトルで、それぞれの要素が、画像がどのひらがならしいかを表わす確率であるようなものである。</p>
<p>このようなベクトルからベクトルへの変換をいわゆる全結合層の連結によって表わすようなネットワークを特に<strong>マルチレイヤ・パーセプトロン</strong>(multilayer perceptron)やMLPと呼ぶ。</p>
<section id="id22">
<h4>全結合層<a class="headerlink" href="#id22" title="Permalink to this heading">#</a></h4>
<p><strong>全結合層</strong> (fully-connected layer)は、入力のベクトル<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>に対して、<strong>重み行列</strong> <span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}^{m \times n}\)</span>と<strong>バイアスベクトル</strong> <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^m\)</span>を使って</p>
<div class="math notranslate nohighlight" id="equation-eq-fully-connected">
<span class="eqno">(3.2)<a class="headerlink" href="#equation-eq-fully-connected" title="この数式へのパーマリンク">#</a></span>\[
\mathbf{y} = \mathbf{W} \mathbf{x} + \mathbf{b}
\]</div>
<p>のように<span class="math notranslate nohighlight">\(\mathbf{y} \in \mathbb{R}^m\)</span>に変換する操作を表わす。従って、全結合層における学習可能なパラメータは<span class="math notranslate nohighlight">\(\mathbf{W}\)</span>と<span class="math notranslate nohighlight">\(\mathbf{b}\)</span>ということになる。</p>
<p>この全結合層が行う操作は<a class="reference internal" href="#equation-eq-fully-connected">(3.2)</a>から分かるように<strong>線形の演算</strong>である。</p>
<p>現在の深層学習においては、以下に示す畳み込みニューラルネットで用いられる畳み込み層など、学習可能なパラメータを含む操作は多くの場合、線形の演算によって定義されることがほとんどである。</p>
</section>
<section id="id23">
<h4>活性化関数<a class="headerlink" href="#id23" title="Permalink to this heading">#</a></h4>
<p>しかし、線形の操作を入力のベクトルに対して何回繰り返したところで、それは所詮線形の操作に他ならない。通常の機械学習において扱う入出力の関係は線形なものばかりではないので、深層学習以前にはカーネル法などを用いて非線形な関係を学習するなどの工夫を取り入れていた。</p>
<p>深層学習においては、学習可能なパラメータを含む演算を線形演算で表わす代わりに非線形の<strong>活性化関数</strong>を用いて、ニューラルネットが表わす入出力データの関係に非線形性を持たせる。</p>
<p>現在の深層学習において、最も広く用いられている活性化関数は<strong>ReLU</strong> (rectified linear unit)と呼ばれるものである。この関数は入力の正の部分だけを残すような関数で、式としては</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f_\text{ReLU}(x) = \begin{cases}
    0 &amp; (x &lt; 0) \\
    x &amp; (\text{otherwise})
\end{cases}
\end{split}\]</div>
<p>のように書ける。</p>
<p>以前は、このような活性化関数として、ソフトな閾値関数であるシグモイド関数が使われていた。シグモイド関数<span class="math notranslate nohighlight">\(\sigma(x)\)</span>は</p>
<div class="math notranslate nohighlight">
\[
\sigma(x) = \frac{1}{1 + \exp(-x)}
\]</div>
<p>のような関数であり、以下のようなグラフを取る。</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;sigmoid function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/0837df3bbe9529e53ea9f7820ca6537afad99d1d859712abdde66f323d6994c2.png" src="../../_images/0837df3bbe9529e53ea9f7820ca6537afad99d1d859712abdde66f323d6994c2.png" />
</div>
</div>
<p>このように、シグモイド関数は入力が0以上の時に1に近い値を、入力が0以下の時に0に近い値を返すようなものであり、これが人間のニューロン同士の結びつきをうまく表わしていると考えられていた。</p>
<p>しかし、実際にニューラルネットの学習を数値計算によって実現しようとする場合、シグモイド関数による活性化は<strong>勾配消失</strong>の問題を引き起こすことが分かった。</p>
<p>勾配消失とは、連鎖律によって、入力の値に関する出力の勾配を求めていく過程で、小さな値が何度もかけ算され、数値誤差により勾配が0になってしまう現象である。シグモイド関数の微分は、</p>
<div class="math notranslate nohighlight">
\[
\sigma'(x) = \frac{e^{-x}{(1 + e^{-x})^2}
\]</div>
<p>なのだが、これは、元のシグモイド関数を用いて、</p>
<div class="math notranslate nohighlight">
\[
\sigma'(x) = \sigma(x) (1 - \sigma(x))
\]</div>
<p>のように書き直せる。シグモイド関数の性質から<span class="math notranslate nohighlight">\(x\)</span>が正負どちらかの方向に大きな値をとれば、1あるいは0に近づいていくため、シグモイド関数の微分<span class="math notranslate nohighlight">\(\sigma'(x)\)</span>は、入力の<span class="math notranslate nohighlight">\(x\)</span>が0から外れた値を取ると、急激に小さくなることが分かる。</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">sigmoid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Derivative of sigmoid&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/69e9b96a60e4204c5db5812b2ebafec6a581d70ea2bd3eedf5050a0316437cd8.png" src="../../_images/69e9b96a60e4204c5db5812b2ebafec6a581d70ea2bd3eedf5050a0316437cd8.png" />
</div>
</div>
<p>この勾配消失により、ニューラルネットの入力に近い側の層において学習が上手く進まないことが長く問題とされてきたが、ReLUは、その導関数が0か1なので、シグモイド関数で問題となっていたような勾配消失の問題が起きづらくなっている。</p>
<p>このような理由から、現在の深層学習においてはReLUおよび、その変形を活性化関数として用いることが多いのである。</p>
</section>
<section id="id24">
<h4>データ正規化<a class="headerlink" href="#id24" title="Permalink to this heading">#</a></h4>
<p>現在の深層学習においては、バッチ正規化 (batch normalization)を始めとしたデータ正規化をネットワーク上に配置することが多い。これは、スケールの異なるデータに対して、ニューラルネットがパラメータを統一的に学習するのに役立つ。</p>
<p>例えば、バッチ正規化の場合、ミニバッチに含まれるデータに対して計算されている特徴の平均と分散をミニバッチ内で計算し、データの平均が0、標準偏差が1となるように正規化を行う。</p>
<p>この効果はバイアスベクトルを例に取ると分かりやすい。もしデータ正規化を行わない場合、ニューラルネットは輝度がとある画像と輝度が一様に持ち上がった画像を区別するために、異なるバイアスベクトルを学習しなければならない。</p>
<p>しかし、データ正規化によってデータの平均値が0になるように正規化されていれば、一様に輝度が持ち上がるなどの変化を学習パラメータの違いによって判別する必要がなくなるため、よりニューラルネットの学習が効率的に進むようになる。</p>
<p>ただし、このようなデータ正規化は正規化を行うデータ群 (バッチ正規化であればミニバッチ)が、元のデータセットに含まれるデータの分布を上手く近似できていることを仮定しているので、ミニバッチのサイズが小さく、データ分布の近似的精度が悪い場合には、学習が逆に上手く進まないこともあるので注意すること。</p>
<p>なお、PyTorchを始めとする深層学習用のライブラリにおいて、データ正規化のモジュール(<code class="docutils literal notranslate"><span class="pre">nn.BatchNorm1d</span></code>や<code class="docutils literal notranslate"><span class="pre">nn.InstanceNorm1d</span></code>など)は単なるバッチ内でのデータの正規化に加えて、さらに平均と標準偏差を調整するようなパラメータを学習可能変数として持っている。</p>
</section>
<section id="id25">
<h4>単純なマルチレイヤ・パーセプトロン<a class="headerlink" href="#id25" title="Permalink to this heading">#</a></h4>
<p>では、ここまでの議論を踏まえて、単純なマルチレイヤ・パーセプトロンを実装してみる。PyTorchにおいては、<strong>学習可能パラメータを含むモジュールはコンストラクタで定義しておく必要がある</strong>ため、以下のコードでは、全結合層を表わす<code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code>と、バッチ正規化を表わす<code class="docutils literal notranslate"><span class="pre">nn.BatchNorm1d</span></code>をコンストラクタの中でインスタンス化しておく。</p>
<p>今回、活性化関数として用いるReLUは学習可能パラメータを持たないので、こちらは、特にコンストラクタでは用意せずに、実際のネットワークが表わす関数の評価処理に対応する<code class="docutils literal notranslate"><span class="pre">forward</span></code>関数の中で<code class="docutils literal notranslate"><span class="pre">torch.relu</span></code>を呼び出す。</p>
<p>理由については後述するが、最終の全結合層に対する活性化関数には単なるソフトマックス関数ではなく、対数ソフトマックス関数を取る方が良い。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    シンプルなマルチレイヤ・パーセプトロン</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<p>また、PyTorchには<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>のサブクラスである<code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code>があり、ニューラルネットを構成するモジュールを引数として与えることで、それを順に実行させることもできる。すると、上記の実装はもう少しすっきりと以下のように書ける。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    nn.Sequentialを継承した例</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">out_features</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>このようにして実装したニューラルネットは入出力の次元数を与えて、以下のようにインスタンス化しておく。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">(</span><span class="mi">48</span> <span class="o">*</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">73</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="ssec-optimizer-preparation">
<span id="id26"></span><h3><span class="section-number">3.5.3. </span>オプティマイザの準備<a class="headerlink" href="#ssec-optimizer-preparation" title="Permalink to this heading">#</a></h3>
<p>やや天下り式ではあるが、今回は、多くの問題に対して、それなりに良い性能を発揮するAdamをオプティマイザに用いる。ニューラルネットの学習可能パラメータは<code class="docutils literal notranslate"><span class="pre">parameters</span></code>関数で得られるので、これをオプティマイザの第一引数に指定する。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.0e-3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="ssec-loss-function">
<span id="id27"></span><h3><span class="section-number">3.5.4. </span>損失関数の設定<a class="headerlink" href="#ssec-loss-function" title="Permalink to this heading">#</a></h3>
<p><strong>損失関数</strong> (loss function)は、ニューラルネットを訓練するための基準を決める関数であり、問題の種類ごとに、おおよそどのような関数を使えば良いかが決まっている。</p>
<p>識別問題の場合には、2クラス分類なら二値交差エントロピー (binary cross entropy)を、多クラス分類なら交差エントロピー(cross entropy)を用いるのが一般的である。これらを<span class="math notranslate nohighlight">\(\mathcal{L}_{\rm BCE}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_{\rm CE}\)</span>と書くことにすると、それぞれ以下の式で表わされる。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathcal{L}_{\rm BCE} &amp;= - y \log x - (1 - y) \log (1- x) \\
\mathcal{L}_{\rm CE} &amp;= - y \log x
\end{align}
\end{split}\]</div>
<p>なお、<span class="math notranslate nohighlight">\(y \in \lbrace 0, 1\rbrace\)</span>, <span class="math notranslate nohighlight">\(x \in [0, 1]\)</span>はそれぞれ正解のラベルと、予測のラベルを表わす。</p>
<p>交差エントロピーには、回帰問題で一般的に用いられる最小二乗誤差などと比べて、ラベルが正解から外れている時に、大きなペナルティが与えられる、という特徴があるため、より分類問題に向いた誤差指標と言える。</p>
<section id="softmax">
<h4>Softmax関数の計算<a class="headerlink" href="#softmax" title="Permalink to this heading">#</a></h4>
<p>さて、ここで一つ重要な問題がある。多クラス分類の場合、予測ラベルは、その値が0から1の範囲に収まるようにソフトマックス関数によって活性化されることが一般的である。活性化前の特徴ベクトルを<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>とすると、活性化後のラベル<span class="math notranslate nohighlight">\(\mathbf{y}\)</span>の各次元<span class="math notranslate nohighlight">\(y_d\)</span>は、以下の式で与えられる。</p>
<div class="math notranslate nohighlight">
\[
y_d = \frac{e^{x_d}}{\sum_{d} e^{x_d}}
\]</div>
<p>この式を見て分かる通り、ソフトマックス関数は分母と分子に指数関数を含むため、<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>のようそが少し大きな値を取るだけで、ニューラルネットワークの学習に一般的に用いられる単精度浮動小数で表せる範囲を超えてしまう。</p>
<p>そのため、実際のソフトマックス関数の計算においては、予め分母と分子を<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>のうち最大の要素を<span class="math notranslate nohighlight">\(\max_j x_j\)</span>として、<span class="math notranslate nohighlight">\(e^{\max_j x_j}\)</span>で割り算をしておく、ということをする。</p>
<p>より具体的には、以下の式によりソフトマックス関数を計算する。</p>
<div class="math notranslate nohighlight">
\[
y_d = \frac{e^{x_d - \max_j x_j}}{\sum_{d} e^{x_d - \max_j x_j}}
\]</div>
<p>この違いを実際に計算して確かめてみよう。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 0-100の間の乱数</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># 単純な計算</span>
<span class="n">softmax0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># 工夫した計算</span>
<span class="n">max_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">softmax1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">max_x</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">max_x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># 結果の表示</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; Input:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Simple:&quot;</span><span class="p">,</span> <span class="n">softmax0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Better:&quot;</span><span class="p">,</span> <span class="n">softmax1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Input: tensor([55.0798, 70.8148, 29.0905, 51.0828, 89.2947, 89.6293, 12.5585, 20.7243,
         5.1467, 44.0810])
Simple: tensor([0., 0., 0., 0., nan, nan, 0., 0., 0., 0.])
Better: tensor([5.7665e-16, 3.9313e-09, 2.9778e-27, 1.0593e-17, 4.1712e-01, 5.8288e-01,
        1.9686e-34, 6.9264e-31, 1.1892e-37, 9.6426e-21])
</pre></div>
</div>
</div>
</div>
<p>いかがだろうか。このように、単純にソフトマックス関数を計算してしまうと、入力の<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>に一つ、大きな値が含まれるだけで、計算に失敗してしまうことが分かる。自分でソフトマックス関数を書く場合には注意されたい (特にNumPyには標準のソフトマックス関数が実装されていない)。</p>
</section>
<section id="softmaxlogsoftmax">
<h4>SoftmaxとLogSoftmax<a class="headerlink" href="#softmaxlogsoftmax" title="Permalink to this heading">#</a></h4>
<p>さて、続いてはソフトマックス関数(softmax)と、対数ソフトマックス関数(log-softmax)の違いについて見ていきたい。前述のニューラルネットワークでは、最終層の活性化関数に対してソフトマックス関数ではなく対数ソフトマックス関数を用いていたが、もちろんこれにも意味がある。</p>
<p>例えば、次の例を見てみてほしい。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">softmax</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Softmax:&quot;</span><span class="p">,</span> <span class="n">softmax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Softmax: tensor([6.4727e-31, 3.0083e-17, 0.0000e+00, 2.1843e-34, 3.3867e-01, 6.6133e-01,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8099e-40])
</pre></div>
</div>
</div>
</div>
<p>この例では、ソフトマックス関数の出力で、単精度浮動小数では表せないような微小な値が出てきてしまい、アンダーフローが起こって、一部の値が0になっていることが分かる。このような出力に対して対数を取ってしまえば、<span class="math notranslate nohighlight">\(-\infty\)</span>のような好ましくない値が得られてしまう。</p>
<p>これは、交差エントロピー誤差の計算に影響を与える。前述の通り、交差エントロピーの計算には対数が含まれるので、<span class="math notranslate nohighlight">\(-\infty\)</span>のような不正な値が入ってくると、誤差関数の計算に失敗してしまうのである。そこで、より数値計算的に安定なやり方で、ソフトマックス関数の対数、即ち対数ソフトマックス関数を求めてしまおう、というのが、最終出力層を対数ソフトマックス関数で活性化している理由である。</p>
<p>ソフトマックス関数に対して対数を取ると、以下のような式になる。</p>
<div class="math notranslate nohighlight">
\[
\log y_d = x_d - \log\left( \sum_{d} e^{x_d}  \right)
\]</div>
<p>この式において、<span class="math notranslate nohighlight">\(x_d\)</span>に大きさのばらつきがあると、<span class="math notranslate nohighlight">\(e^{x_d}\)</span>の値はさらに大小差が大きくなり、数値計算においては、その和を取ったときに<strong>桁落ち誤差</strong>が起こって、相対的に小さな値が無視されてしまう。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 桁落ち誤差の例</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.00001</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">10000.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[10000.]
</pre></div>
</div>
</div>
</div>
<p>このような桁落ち誤差を防ぐために、ある数列の「指数の和」の「対数」を計算するときには、一工夫必要になる。具体的には、先ほどのソフトマックス関数の計算の時と同様に、各<span class="math notranslate nohighlight">\(x_d\)</span>から、要素の最大値<span class="math notranslate nohighlight">\(\max_j x_j\)</span>を引き算しておく、ということをする。すなわち、</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\log y_d &amp;= x_d - \log\left( \sum_{d} e^{x_d - \max_j x_j}  \right) + \log e^{\max_j x_j} \\
&amp;= x_d - \log\left( \sum_{d} e^{x_d - \max_j x_j}  \right) + \max_j x_j
\end{align}
\end{split}\]</div>
<p>のように計算を行なう。すると、各<span class="math notranslate nohighlight">\(e^{x_d - \max_j x_j}\)</span>は0から1の間の値を取るため、桁落ち誤差の影響を抑えることができる。このような計算が<code class="docutils literal notranslate"><span class="pre">F.softmax</span></code>や<code class="docutils literal notranslate"><span class="pre">nn.Softmax</span></code>の中では行なわれており、計算結果を比較すると、以下のように<span class="math notranslate nohighlight">\(-\infty\)</span>のような不正な値を影響を受けることなく計算が行なわれていることが分かる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logsoftmax0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">logsoftmax1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Simple:&quot;</span><span class="p">,</span> <span class="n">logsoftmax0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Better:&quot;</span><span class="p">,</span> <span class="n">logsoftmax1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Simple: tensor([-69.5126, -38.0426,     -inf, -77.5066,  -1.0827,  -0.4135,     -inf,
            -inf,     -inf, -91.5102])
Better: tensor([ -69.5126,  -38.0426, -121.4912,  -77.5066,   -1.0827,   -0.4135,
        -154.5551, -138.2235, -169.3787,  -91.5102])
</pre></div>
</div>
</div>
</div>
<p>さて、対数ソフトマックス関数の出力を<span class="math notranslate nohighlight">\(z\)</span>とすれば、交差エントロピーは</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\rm CE} = -y z
\]</div>
<p>のように書き直せる。この関数こそが非負対数尤度誤差 (non-negative log-likelihood)であり、PyTorchでは<code class="docutils literal notranslate"><span class="pre">nn.NNLLoss</span></code>として用意されている。</p>
<p>以上の議論から、より高精度な識別結果を得るためには、</p>
<ul class="simple">
<li><p>最終層を対数ソフトマックス関数 (<code class="docutils literal notranslate"><span class="pre">nn.LogSoftmax</span></code> or <code class="docutils literal notranslate"><span class="pre">F.log_softmax</span></code>)で活性化する</p></li>
<li><p>損失関数に非負対数尤度誤差 (<code class="docutils literal notranslate"><span class="pre">NNLLoss</span></code>)を用いる</p></li>
</ul>
<p>という工夫を行なうのが良い。</p>
<p>PyTorchにおいては、慣習的に<code class="docutils literal notranslate"><span class="pre">criterion</span></code>という変数に損失関数を取ることが多く、それに倣って、以下のように<code class="docutils literal notranslate"><span class="pre">NNLLoss</span></code>クラスをインスタンス化しておく。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 損失関数の準備</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="ssec-training-loop">
<span id="id28"></span><h3><span class="section-number">3.5.5. </span>トレーニング・ループ<a class="headerlink" href="#ssec-training-loop" title="Permalink to this heading">#</a></h3>
<p>さて、ここまで準備ができたら、最後にニューラルネットワークを訓練するための繰り返し計算をforループによって実装しよう。</p>
<p>深層学習においては、トレーニングデータを何周分トレーニングするかを<strong>エポック</strong>という用語で表わす。以下の例では<span class="output text_plain">3</span>周分、すなわち<span class="output text_plain">3</span>エポックの学習を行なっている。</p>
<p>学習と同時に、進行状況が分かるようにしておくことはとても大事で、以下の例では<code class="docutils literal notranslate"><span class="pre">tqdm</span></code>モジュールを用いて、訓練の進み具合と、その時の損失関数の値、ならびに識別精度を表示するようにしている。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># トレーニング・ループ</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># 進行状況の可視化</span>
    <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="c1"># 訓練データの取り出し</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># 推定と損失関数の評価</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_true</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>
            <span class="s2">&quot;[MLP] epoch=</span><span class="si">{:d}</span><span class="s2">, loss=</span><span class="si">{:1.3f}</span><span class="s2">, acc=</span><span class="si">{:1.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">acc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># 誤差逆伝搬によるパラメータの更新</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "96c45353b55d4570a80554005bbede95", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "29f0c124ae01466dbe5b057edaab2150", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "ddf8d98dcd72441090a275b56df23da1", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>上記の学習について、誤差と精度の変化をプロットしてみる。そのままだと、上下の振動が大きく見づらいので、移動平均を取って曲線を滑らかにしておく。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 移動平均を取る (= ボックス・フィルタをかける)</span>
<span class="n">box_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">box</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">box_size</span><span class="p">))</span> <span class="o">/</span> <span class="n">box_size</span>

<span class="n">losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">box</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
<span class="n">accuracies</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">accuracies</span><span class="p">,</span> <span class="n">box</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)),</span> <span class="n">losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color_palette</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Steps&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)),</span> <span class="n">accuracies</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color_palette</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/7af69cbe434e5fd6552fa438a57cd94ea7a71f080521ee58178afeebd9fd06fe.png" src="../../_images/7af69cbe434e5fd6552fa438a57cd94ea7a71f080521ee58178afeebd9fd06fe.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
<span class="n">n_succ</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">n_succ</span> <span class="o">+=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_true</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Acc: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_succ</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "d1ba50ad0cae43b382c818d48f6bb28d", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Acc: 0.760
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id29">
<h2><span class="section-number">3.6. </span>畳み込みニューラルネットによる学習<a class="headerlink" href="#id29" title="Permalink to this heading">#</a></h2>
<p>ここまでは、全結合層からなるマルチレイヤ・パーセプトロンによる学習を見てきたが、今回は取り扱う対象が画像であるため、<strong>畳み込みニューラルネット</strong>により、より効果的な学習が期待できる。</p>
<p>全結合層は、入力のベクトルに対して、行列を作用させて、さらにその結果にバイアスベクトルを加算する、というものであった。この操作は、畳み込みニューラルネットで用いられる<strong>畳み込み層</strong>においてもほとんど同じである。</p>
<p>今、入力が画像であり、それが画素ごとに特徴化されて<span class="math notranslate nohighlight">\((H, W, D)\)</span>という大きさを持つデータであるとしよう。なお、<span class="math notranslate nohighlight">\(H\)</span>は画像 (特徴マップ)の高さ、<span class="math notranslate nohighlight">\(W\)</span>は幅、<span class="math notranslate nohighlight">\(D\)</span>は各画素が持つ特徴の次元である。</p>
<p>このデータを畳み込み層によって<span class="math notranslate nohighlight">\((W', H', D')\)</span>に変換することを考える。畳み込み層が学習可能な重みが畳み込みカーネルを表わす二次元のマップであり、これが<span class="math notranslate nohighlight">\(D \times D'\)</span>個用意される。カーネルのサイズを<span class="math notranslate nohighlight">\(K\times K\)</span>とする場合、畳み込み前後の特徴マップのサイズには、</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
W' &amp;= W - K + 1 \\
H' &amp;= H - K + 1
\end{align}
\end{split}\]</div>
<p>という関係がある。また、畳み込み層は、カーネルの大きさに加えて、何画素飛ばしでカーネルを適用するかを表わすストライド<span class="math notranslate nohighlight">\(S\)</span>と、画像の周りを何らかの値で埋めて、大きさを調整するパディング<span class="math notranslate nohighlight">\(P\)</span>をパラメータとして持つ。これらの値を加味すると、畳み込み前後の画像サイズの関係は、以下のように書き直せる。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
W' = \frac{H - K + 2P}{S} + 1 \\
H' = \frac{H - K + 2P}{S} + 1
\end{align}
\end{split}\]</div>
<p>現在は、ニューラルネットワークの畳み込み層で、画像や特徴マップのサイズを変更することは少なく、多くの場合、<span class="math notranslate nohighlight">\(P=(K-1)/2\)</span>, <span class="math notranslate nohighlight">\(S=1\)</span>とすることで、畳み込み前後のサイズが変わらないようにすることが多い。例えば、<span class="math notranslate nohighlight">\(3\times 3\)</span>の畳み込みを用いる場合、<span class="math notranslate nohighlight">\(P=1\)</span>, <span class="math notranslate nohighlight">\(S=1\)</span>と設定する。</p>
<p>その代わり、画像や特徴マップのサイズを変更する操作としてプーリングという操作を行なう。プーリングは、<span class="math notranslate nohighlight">\(2 \times 2\)</span>などの小さな画像領域において、その画素が持つ特徴の最大値や平均を取るような操作を指す。PyTorchにおいては、最大を取る操作が<code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d</span></code>および<code class="docutils literal notranslate"><span class="pre">F.max_pool2d</span></code>に、平均を取る操作が<code class="docutils literal notranslate"><span class="pre">nn.AvgPool2d</span></code>および<code class="docutils literal notranslate"><span class="pre">F.avg_pool2d</span></code>に用意されている。</p>
<p>これらを用いて簡単な畳み込みニューラルネットを実装したものが以下である。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    畳み込みニューラルネット</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="mi">6</span> <span class="o">*</span> <span class="mi">16</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>  <span class="c1"># (B, 48, 48 64)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (B, 24, 24, 64)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>  <span class="c1"># (B, 24, 24, 32)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (B, 12, 12, 32)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>  <span class="c1"># (B, 12, 12, 16)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (B, 6, 6, 16)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># (B, 6 * 6 * 16)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, 73)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<p>ところで、これまで、学習はCPUを用いて計算してきたが、上記の畳み込みニューラルネットになってくると、CPUだけの計算では少々時間がかかるようになってくる。</p>
<p>そこで、GPUが使える環境ではGPUを使うようにデバイスの設定を行なう。GPU上ではNVIDIA社のCUDA (compute unified device architecture)を用いて計算が行なわれ、GPUの性能にもよるが、CPUより遙かに高速な学習が可能である。</p>
<p>CUDAが使えるかどうかの判定には<code class="docutils literal notranslate"><span class="pre">torch.cuda.is_available()</span></code>関数を用いる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># デバイスの判定</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Your device is&quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Your device is cpu
</pre></div>
</div>
</div>
</div>
<p>デバイスが取得できたら、ネットワークならびに学習データを、デバイスに転送する操作が必要になる。具体的には、それぞれに用意された<code class="docutils literal notranslate"><span class="pre">to(...)</span></code>という関数に対して、上記の<code class="docutils literal notranslate"><span class="pre">device</span></code>インスタンスを指定する。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ネットワークおよびオプティマイザのインスタンス化</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CNN</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">73</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.0e-3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="c1"># データのデバイスへの転送</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># 推論ならびに損失関数の評価</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_true</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s2">&quot;[CNN] loss=</span><span class="si">{:1.3f}</span><span class="s2">, acc=</span><span class="si">{:1.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">acc</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>

        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "cc672d2aebfd49b8ae5fc1a48e36dcae", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "8681cb10fd154645860aefcc3a7e34a3", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "e8eedeb3de4c4bd18bc328e24f0443a7", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 移動平均を取る (= ボックス・フィルタをかける)</span>
<span class="n">box_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">box</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">box_size</span><span class="p">))</span> <span class="o">/</span> <span class="n">box_size</span>

<span class="n">losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">box</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
<span class="n">accuracies</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">accuracies</span><span class="p">,</span> <span class="n">box</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)),</span> <span class="n">losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color_palette</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Steps&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)),</span> <span class="n">accuracies</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color_palette</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">0.88</span><span class="p">,</span> <span class="mf">0.58</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9d93fc11144cd6a353a03428d9d923ffca14e1d9261079464009a5d613ed82a6.png" src="../../_images/9d93fc11144cd6a353a03428d9d923ffca14e1d9261079464009a5d613ed82a6.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
<span class="n">n_succ</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">n_succ</span> <span class="o">+=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_true</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Acc: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_succ</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "d22a810471494eb8a301c6348337a061", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Acc: 0.910
</pre></div>
</div>
</div>
</div>
</section>
<section id="id30">
<h2><span class="section-number">3.7. </span>学習結果の保存<a class="headerlink" href="#id30" title="Permalink to this heading">#</a></h2>
<p>実際に深層学習をアプリケーションで使う際には、予め時間を掛けてニューラルネットワークを学習しておき、その学習結果だけを読み出して、応用に使用する場合がほとんどだろう。また、途中まで学習がされているネットワークを読み出して、そこから別のデータで<strong>ファイン・チューニング</strong>を行なう場合もあるだろう。</p>
<p>そのような時には、ネットワークのパラメータを<code class="docutils literal notranslate"><span class="pre">*.pth</span></code>ファイルに保存しておくことが一般的である。ネットワークやオプティマイザには、<code class="docutils literal notranslate"><span class="pre">state_dict()</span></code>関数が用意されていて、パラメータとその名前を格納した辞書を取得することができる。このような辞書がネストしたものを<code class="docutils literal notranslate"><span class="pre">torch.save</span></code>関数に渡すことで重みを保存することができる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ckpt</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s2">&quot;optim&quot;</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
<span class="p">}</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">ckpt</span><span class="p">,</span> <span class="s2">&quot;ckpt.pth&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>このようにして、保存したパラメータは<code class="docutils literal notranslate"><span class="pre">torch.load</span></code>関数で辞書型として読み出すことができ、対応する辞書を<code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code>関数に指定することでパラメータを上書きすることができる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ネットワークとオプティマイザの別インスタンスを作成</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">CNN</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">73</span><span class="p">)</span>
<span class="n">model2</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optim2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model2</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.0e-3</span><span class="p">)</span>

<span class="c1"># 学習済みパラメータの読み込み</span>
<span class="n">ckpt2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;ckpt.pth&quot;</span><span class="p">)</span>
<span class="n">model2</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">ckpt2</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">])</span>
<span class="n">optim2</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">ckpt2</span><span class="p">[</span><span class="s2">&quot;optim&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>再度、読み込んだパラメータを用いて性能を確認してみる。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
<span class="n">n_succ</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">model2</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model2</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">n_succ</span> <span class="o">+=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_true</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Acc: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_succ</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "a70dba1e5d864b899a9c20feb28d7d3f", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Acc: 0.910
</pre></div>
</div>
</div>
</div>
<p>このように、パラメータの読み込みにより、以前の結果を再現できていることが分かる。</p>
<div class="warning admonition">
<p class="admonition-title">異なるデバイスでのパラメータの読み込み</p>
<p>学習を行ったデバイスと異なるデバイスで<code class="docutils literal notranslate"><span class="pre">.pth</span></code>ファイルを読み込む場合、単に <code class="docutils literal notranslate"><span class="pre">torch.load</span></code>を呼び出すだけでは<code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>になってしまう。</p>
<p>これを防ぐには<code class="docutils literal notranslate"><span class="pre">torch.load</span></code>の引数に<code class="docutils literal notranslate"><span class="pre">map_location=...</span></code>を指定して、どの種類のデバイスに読み込むのかを指定する。例えばCUDA上で学習を行って得たパラメータをCPU上で読み込む場合には、以下のようにすれば良い。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ckpt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;ckpt.pth&quot;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</section>
<section id="ssec-avoid-overfit">
<span id="id31"></span><h2><span class="section-number">3.8. </span>過学習を防ぐための工夫<a class="headerlink" href="#ssec-avoid-overfit" title="Permalink to this heading">#</a></h2>
<p>上記のCNNによる文字分類の結果を見てみると、訓練時の精度と比較して、テスト時の精度がやや劣っていることが分かる。このような訓練データに対して、過度に高い精度が出てしまう現象を<strong>過学習</strong>と呼ぶ。</p>
<p>過学習を防ぐための方法にはいくつかあるが、主なものとして以下の4つが挙げられる。</p>
<ul class="simple">
<li><p>学習可能なパラメータの数を減らす</p></li>
<li><p>訓練データ数を増やす</p></li>
<li><p>正則化項の追加</p></li>
<li><p>ドロップアウトの導入</p></li>
</ul>
<p>学習パラメータの数は、過学習の主要な原因の一つで、これはニューラルネットに限らず、モデルが複雑になればなるほど、必要な学習データの数が増す。これは、単純には、連立方程式における制約式の数と未知数の数の関係と同じであり、パラメータ数が多ければ制約を増やす意味で多くの訓練データが必要であり、訓練データの量が十分でないときには、より簡素な機械学習モデルを使う方が過学習の影響を抑えられる。</p>
<p>故に、どのような問題に対しても、<strong>無差別に深層学習を適用すれば良い結果が得られるわけではない</strong>、という点には最大の注意を払ってほしい。やはり、深層学習が発展した今でも、問題に応じて適切な手法を選ばなければならないことに変わりはない。</p>
<p>また、正則化項の導入により過学習を防ぐことも可能である。正則化項とは、非常に大雑把な議論では、各パラメータの絶対値が大きくなりすぎないようにペナルティ項を追加することに対応する。このようなペナルティ項には様々な種類があるが、よく用いられるのはL1正則化、ならびにL2正則化である。L1正則化は学習可能パラメータの絶対値の和を使い、L2正則化は学習可能パラメータの二乗の和を用いる。</p>
<p>PyTorchを用いる場合、L2正則化であれば、オプティマイザをインスタンス化する際に引数として<code class="docutils literal notranslate"><span class="pre">weight_decay=...</span></code>というパラメータを指定することで、正則化がかかる。例えば、</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.0e-3</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1.0e-6</span><span class="p">)</span>
</pre></div>
</div>
<p>といった感じで、この場合には、パラメータの二乗和に対して<code class="docutils literal notranslate"><span class="pre">weight_decay=...</span></code>で指定した値が乗算されたものが損失関数に追加される。</p>
<p>一方、L1正則化を行ないたい場合には、陽にパラメータの大きさの絶対値の和を足し上げていく必要がある。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># L1正則化の実装例</span>
<span class="n">l1_reg</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">l1_reg</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>最後に紹介するDropoutは、全結合層や畳み込み層中のチャネル間の結びつきをランダムに無効化しながら学習するという仕組みである。例えば、全結合層により、ベクトルの次元数を<span class="math notranslate nohighlight">\(D_1\)</span>から<span class="math notranslate nohighlight">\(D_2\)</span>に変える場合、Dropoutされる確率を<span class="math notranslate nohighlight">\(p \in (0, 1)\)</span>として、<span class="math notranslate nohighlight">\(p D_1 D_2\)</span>個の行列要素を<strong>訓練時のみ</strong>ランダムに0で埋めてしまう。</p>
<p>こうすることにより、機械学習モデルは、どのパラメータを使った場合にも、まんべんなく訓練データに対する予測ができるように学習が進み、結果として過学習を防ぐことができる。</p>
<p>PyTorchで実装する場合には、活性化関数の後に<code class="docutils literal notranslate"><span class="pre">nn.Dropout</span></code>あるいは<code class="docutils literal notranslate"><span class="pre">F.dropout</span></code>を追加すれば良い。ただし、PyTorchの<code class="docutils literal notranslate"><span class="pre">Dropout</span></code>に指定する確率は<strong>パラメータを0で埋める割合</strong>であることに注意すること (つまり<code class="docutils literal notranslate"><span class="pre">Dropout(p=0.0)</span></code>とすると、何もしないことと同義になる)。</p>
<div class="note admonition">
<p class="admonition-title">レイヤーの順序</p>
<p>現在のニューラルネットワークにおいては、</p>
<ul class="simple">
<li><p>全結合層、畳み込み層などの学習可能な線形操作</p></li>
<li><p>バッチ正規化などのデータ正規化</p></li>
<li><p>活性化関数</p></li>
<li><p>ドロップアウト</p></li>
</ul>
<p>のような順序で演算を行なうことが多いが、この善し悪しについてはあまりはっきりとしないところがある。特に、データ正規化と活性化関数の順序については、その演算の意味を考えると、順序が逆の方が良いと思える部分も多い。</p>
<p>データ正規化の後に活性化関数をもってくる、という順序はバッチ正規化の原論文である<span id="id32">[<a class="reference internal" href="#id36" title="Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, 448–456. pmlr, 2015.">Ioffe and Szegedy, 2015</a>]</span>で提唱された順序であり、多くの手法がこの順序を採用している。しかし、データ正規化がデータの平均を0に合わせることを考えると、その後に、負の値を0で埋めてしまうReLUを適用するのは、やや不適切に思えなくもない。</p>
<p>さらに言えば、データ正規化の効果は全結合層などの線形操作においてバイアスベクトルの学習を促進する効果にあるわけだから、線形操作の直前にデータ正規化が行なわれる方が自然である。</p>
<p>実際、「線形操作」→「活性化関数」→「ドロップアウト」→「データ正規化」の順序の方が性能が向上するという見方もある。このように、論文に書かれていることが常に正しいとは限らないので、論文を読むときには、多少は疑いの目をもって読むことが大事である。</p>
</div>
</section>
<section id="id33">
<h2><span class="section-number">3.9. </span>練習問題<a class="headerlink" href="#id33" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>上記のニュートン法により得られた関数最小化の軌跡を、単純な<a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_descent">最急降下法</a>ならびに一階導関数だけを用いてHesse行列を近似する<a class="reference external" href="https://en.wikipedia.org/wiki/Quasi-Newton_method">準ニュートン法</a>と比較せよ。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code>のサブクラスとしてシグモイド関数を扱うクラスを実装してみよ。シグモイド関数の導関数は自分自身の値を用いて表せることに注意すること。</p></li>
<li><p>MNISTの例について、オプティマイザの種類によって、誤差関数の収束と識別精度の上昇がどのように変化するかを調査せよ。</p></li>
<li><p><a class="reference internal" href="#ssec-avoid-overfit"><span class="std std-ref">過学習を防ぐための工夫</span></a>に示した方法によって、どの程度、過学習が抑制できるかを実際に試してみよ。</p></li>
</ol>
</section>
<section id="id34">
<h2><span class="section-number">3.10. </span>参考文献<a class="headerlink" href="#id34" title="Permalink to this heading">#</a></h2>
<div class="docutils container" id="id35">
<dl class="citation">
<dt class="label" id="id45"><span class="brackets"><a class="fn-backref" href="#id3">Fuk80</a></span></dt>
<dd><p>Kunihiko Fukushima. Neocognitron: a self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. <em>Biological Cybernetics</em>, 36(4):193–202, apr 1980. <a class="reference external" href="https://doi.org/10.1007/bf00344251">doi:10.1007/bf00344251</a>.</p>
</dd>
<dt class="label" id="id62"><span class="brackets"><a class="fn-backref" href="#id15">GMB14</a></span></dt>
<dd><p>Caglar Gulcehre, Marcin Moczulski, and Yoshua Bengio. Adasecant: robust adaptive secant method for stochastic gradient. In <em>IEEE International Joint Conference on Neural Network</em>. 2014.</p>
</dd>
<dt class="label" id="id36"><span class="brackets"><a class="fn-backref" href="#id32">IS15</a></span></dt>
<dd><p>Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. In <em>International conference on machine learning</em>, 448–456. pmlr, 2015.</p>
</dd>
<dt class="label" id="id44"><span class="brackets"><a class="fn-backref" href="#id4">KSH12</a></span></dt>
<dd><p>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In <em>Proceedings of the 25th International Conference on Neural Information Processing Systems</em>, 1097–1105. 2012. <a class="reference external" href="https://doi.org/10.1145/3065386">doi:10.1145/3065386</a>.</p>
</dd>
</dl>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents/sec2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="feature-extraction.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">前へ</p>
        <p class="prev-next-title"><span class="section-number">2. </span>特徴量抽出</p>
      </div>
    </a>
    <a class="right-next"
       href="exercise-ogura.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">次へ</p>
        <p class="prev-next-title"><span class="section-number">4. </span>演習2 - 百人一首エージェントを作る</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> 目次
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">3.1. 深層学習の歴史</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">3.2. PyTorchの基本</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-tensor">3.2.1. torch.Tensor</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">torch.Tensorを使う</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">torch.tensorを使う</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-from-numpy">torch.from_numpy</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">3.2.2. torch.Tensorからの値の取り出し</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">3.3. 自動微分</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">3.3.1. 自動微分の仕組み</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">3.3.2. 自動微分の利用</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">3.3.3. 自動微分可能な演算の定義</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">3.3.4. 二階微分の計算</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">3.3.5. 多変数関数の微分</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">3.4. ニュートン法の実装</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">3.4.1. オプティマイザを利用した最適化</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">確率的最急降下法</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rmsprop">RMSprop</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#adam">Adam</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">オプティマイザを使用した最適化</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">3.5. 深層学習</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssec-dataloader-preparation">3.5.1. データローダの作成</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssec-network-architecture">3.5.2. ネットワークの構築</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">全結合層</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">活性化関数</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">データ正規化</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">単純なマルチレイヤ・パーセプトロン</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssec-optimizer-preparation">3.5.3. オプティマイザの準備</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssec-loss-function">3.5.4. 損失関数の設定</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax関数の計算</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#softmaxlogsoftmax">SoftmaxとLogSoftmax</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ssec-training-loop">3.5.5. トレーニング・ループ</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">3.6. 畳み込みニューラルネットによる学習</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">3.7. 学習結果の保存</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ssec-avoid-overfit">3.8. 過学習を防ぐための工夫</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id33">3.9. 練習問題</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id34">3.10. 参考文献</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
著者 Tatsuya Yatagawa
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright CC BY-NC-SA 4.0, 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>