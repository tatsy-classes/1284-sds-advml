{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(sec:exercise-othello)=\n",
    "# 演習3 - オセロエージェントを作る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "今回の演習では、前節、[基本のオセロAI](sec:othello-agent)の延長として、より強いオセロのAIを作成してみよう。\n",
    "\n",
    "前節では、オセロAIの基本であるゲーム木の探索方法について、マス評価値に基づいた[ミニマックス探索](ssec:minimax)と、マス評価値に依らない[原始モンテカルロ探索](ssec:monte-carlo)について解説した。\n",
    "\n",
    "本演習では、これらの延長として、Q関数の**関数近似**に基づいたAIの強化に取り組んでみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# joblib用のtqdm\n",
    "import contextlib\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(total=None, **kwargs):\n",
    "    pbar = tqdm(total=total, miniters=1, smoothing=0, **kwargs)\n",
    "\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            pbar.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "\n",
    "    try:\n",
    "        yield pbar\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Q学習における関数近似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**関数近似**とは、Q関数$Q(s, a)$を何らかの機械学習モデルによって近似する手法を指す。[Q学習の基本](sec:q-learning)で紹介した手法ではQテーブルを離散化していたが、このような手法は、状態数の多いゲームでは適用が難しい。\n",
    "\n",
    "今回、演習で扱うオセロも取り得る盤の状態は、64個のマスについて、黒、白、空の3通りが考えられるから、その状態数は$3^{64} \\approx 3 \\times 10^{30}$にもなる。従って、このような大量の状態数に対して、そのまま離散的なQテーブルを定義することは現実的ではない。\n",
    "\n",
    "そこで、状態$s$を入力とし、取り得る行動$a \\in \\mathcal{A}$について、行動価値を与えるような関数$Q(s, a)$を機械学習モデルによって表現することを考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "最も単純な例として、状態を表わすパラメータをベクトル$\\mathbf{s}$として表わし、この線形変換により、各行動の価値を要素に持つベクトル$\\mathbf{a}$を求めるようなモデルを考えることができる。\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = \\mathbf{W} \\mathbf{s} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "このような線形モデルであれば、Q学習の更新式:\n",
    "\n",
    "$$\n",
    "Q_{\\rm new}(s, a) = Q(s,a ) + \\alpha \\left[ R(s, a) + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "の第2項に現れるTD誤差を最小化するようにモデルのパラメータ$\\mathbf{W}$, $\\mathbf{b}$を最適化すれば良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### scikit-learnを用いた関数近似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import othello\n",
    "from othello import Move, Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = othello.make()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "reg = MultiOutputRegressor(SGDRegressor())\n",
    "# reg = MLPRegressor(learning_rate_init=0.01)\n",
    "dummy_X = [np.random.random(size=(8 * 8))]\n",
    "dummy_y = [np.random.random(size=(8 * 8 + 1))]\n",
    "reg.partial_fit(dummy_X, dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d8eb18ea7f4755be021db1c9c97111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "epsilon = 0.5\n",
    "\n",
    "for _ in tqdm(range(100)):\n",
    "    X = []\n",
    "    y = []\n",
    "    player, board = env.reset()\n",
    "    while not env.gameset():\n",
    "        # 入力のベクトルは現在の状態\n",
    "        feature = player * board.reshape((-1))\n",
    "        X.append(feature.copy())\n",
    "    \n",
    "        # 手を指して状態を進める\n",
    "        # 以下はε-greedy法で手を選ぶ\n",
    "        moves = env.legal_moves(player)\n",
    "        if len(moves) == 0:\n",
    "            move = Move.Pass(player)\n",
    "        elif np.random.uniform(0.0, 1.0) < epsilon:\n",
    "            move = np.random.choice(moves)\n",
    "        else:\n",
    "            action_values = reg.predict([feature])[0]\n",
    "            best_move = None\n",
    "            best_score = -np.inf\n",
    "            for m in moves:\n",
    "                action_index = m.y * 8 + m.x\n",
    "                if best_score < action_values[action_index]:\n",
    "                    best_score = action_values[action_index]\n",
    "                    best_move = m\n",
    "            move = best_move\n",
    "                \n",
    "        next_player, next_board = env.step(move)\n",
    "    \n",
    "        # 対戦結果に応じて報酬(罰)を与える\n",
    "        reward = 0.0\n",
    "        if env.gameset():\n",
    "            n_me = env.count(player)\n",
    "            n_you = env.count(next_player)\n",
    "            reward = n_me - n_you          \n",
    "    \n",
    "        # 次の状態に対する最も良いQ値を得る\n",
    "        feature = next_player * next_board.reshape((-1))\n",
    "        action_values = reg.predict([feature])[0]\n",
    "        best_score = np.max(action_values)\n",
    "    \n",
    "        action_index = 64\n",
    "        if not move.is_pass():\n",
    "            action_index = move.y * 8 + move.x\n",
    "    \n",
    "        target = action_values.copy()\n",
    "        target[action_index] = reward + gamma * best_score\n",
    "        y.append(target)\n",
    "    \n",
    "        # 変数を更新\n",
    "        player = next_player\n",
    "        board = next_board\n",
    "\n",
    "    # 1エピソードごとに回帰モデルを更新\n",
    "    reg.partial_fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def move_by_random(env, player):\n",
    "    \"\"\"有効手の中からランダムに手を選ぶ\"\"\"\n",
    "    moves = env.legal_moves(player)\n",
    "    if len(moves) == 0:\n",
    "        return Move.Pass(player)\n",
    "    else:\n",
    "        return np.random.choice(moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9348b4ecc648eea06bcbe785564e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_jobs = 4\n",
    "n_episodes = 1000\n",
    "\n",
    "def match():\n",
    "    # ゲームのリセット\n",
    "    env = othello.make()\n",
    "    player, board = env.reset()\n",
    "\n",
    "    # エピソード開始\n",
    "    while not env.gameset():\n",
    "        # 黒番:\n",
    "        if player == Player.BLACK:\n",
    "            # セル評価値が最も高い場所に着手する\n",
    "            moves = env.legal_moves(player)\n",
    "\n",
    "            if len(moves) == 0:\n",
    "                move = Move.Pass(player)\n",
    "            else:\n",
    "                best_move = None\n",
    "                best_score = -np.inf\n",
    "                feature = player * board.reshape((-1))\n",
    "                action_values = reg.predict([feature])[0]\n",
    "                for m in moves:\n",
    "                    action_index = m.y * 8 + m.x                    \n",
    "                    if best_score < action_values[action_index]:\n",
    "                        best_score = action_values[action_index]\n",
    "                        best_move = m\n",
    "\n",
    "                move = best_move\n",
    "        # 白番:\n",
    "        if player == Player.WHITE:\n",
    "            # 着手可能な手があればランダムに1つを選ぶ\n",
    "            move = move_by_random(env, player)\n",
    "\n",
    "        # 着手による盤の状態の更新\n",
    "        player, board = env.step(move)\n",
    "\n",
    "    return env.count()\n",
    "\n",
    "\n",
    "with tqdm_joblib(n_episodes):\n",
    "    result = joblib.Parallel(n_jobs=n_jobs)(\n",
    "        (joblib.delayed(match)() for _ in range(n_episodes)),\n",
    "    )\n",
    "\n",
    "result = np.array(result, dtype=\"int32\")\n",
    "b_win = np.sum(result[:, 0] > result[:, 1])\n",
    "w_win = np.sum(result[:, 0] < result[:, 1])\n",
    "draw = np.sum(result[:, 0] == result[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635 328 37\n"
     ]
    }
   ],
   "source": [
    "print(b_win, w_win, draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 演習内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "今回の演習では以下のようなオセロをプレイするエージェントのクラスを実装する。ただし、基本的にはオセロ環境が渡されてくる`Agent.play`だけを編集すれば良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def play(self, player, env) -> Move:\n",
    "        \"\"\"\n",
    "        この関数を更新、以下はランダムに着手する例\n",
    "        \"\"\"\n",
    "        moves = env.legal_moves()\n",
    "        if len(moves) != 0:\n",
    "            return Move.Pass(player)\n",
    "        else:\n",
    "            return np.random.choice(moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 対戦相手のレベル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- **レベル1:** セル評価値を用いたアルファベータ探索で2手先まで読むAI\n",
    "- **レベル2:** 上記のscikit-learnを用いた関数近似によって、次の手の価値を評価するAI\n",
    "- **レベル3:** Q関数の学習方法を改良し、さらにQ関数に基づいて2手先まで読むAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### ローカル環境でのテスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 本番環境でのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
