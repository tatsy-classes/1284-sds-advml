{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(sec:exercise-othello)=\n",
    "# 演習3 - オセロエージェントを作る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "今回の演習では、前節、[基本のオセロAI](sec:othello-agent)の延長として、より強いオセロのAIを作成してみよう。\n",
    "\n",
    "前節では、オセロAIの基本であるゲーム木の探索方法について、マス評価値に基づいた[ミニマックス探索](ssec:minimax)と、マス評価値に依らない[原始モンテカルロ探索](ssec:monte-carlo)について解説した。\n",
    "\n",
    "本演習では、これらの延長として、Q関数の**関数近似**に基づいたAIの強化に取り組んでみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# グラフの設定\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 150\n",
    "sns.set(style=\"white\", palette=\"colorblind\")\n",
    "\n",
    "# シードの固定\n",
    "random.seed(31415)\n",
    "np.random.seed(31415)\n",
    "\n",
    "# 一部の警告を無視\n",
    "warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from myst_nb import glue\n",
    "except ImportError:\n",
    "    glue = lambda *args, **kwargs: _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# joblib用のtqdm\n",
    "import contextlib\n",
    "\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(total=None, **kwargs):\n",
    "    pbar = tqdm(total=total, miniters=1, smoothing=0, **kwargs)\n",
    "\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            pbar.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "\n",
    "    try:\n",
    "        yield pbar\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Q学習における関数近似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**関数近似**とは、Q関数$Q(s, a)$を何らかの機械学習モデルによって近似する手法を指す。[Q学習の基本](sec:q-learning)で紹介した手法ではQテーブルを離散化していたが、このような手法は、状態数の多いゲームでは適用が難しい。\n",
    "\n",
    "今回、演習で扱うオセロも取り得る盤の状態は、64個のマスについて、黒、白、空の3通りが考えられるから、その状態数は$3^{64} \\approx 3 \\times 10^{30}$にもなる。従って、このような大量の状態数に対して、そのまま離散的なQテーブルを定義することは現実的ではない。\n",
    "\n",
    "そこで、状態$s$を入力とし、取り得る行動$a \\in \\mathcal{A}$について、行動価値を与えるような関数$Q(s, a)$を機械学習モデルによって表現することを考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "最も単純な例として、状態を表わすパラメータをベクトル$\\mathbf{s}$として表わし、この線形変換により、各行動の価値を要素に持つベクトル$\\mathbf{a}$を求めるようなモデルを考えることができる。\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = \\mathbf{W} \\mathbf{s} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "このような線形モデルであれば、Q学習の更新式:\n",
    "\n",
    "$$\n",
    "Q_{\\rm new}(s, a) = Q(s,a ) + \\alpha \\left[ R(s, a) + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "の第2項に現れるTD誤差を最小化するようにモデルのパラメータ$\\mathbf{W}$, $\\mathbf{b}$を最適化すれば良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### scikit-learnを用いた関数近似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import othello\n",
    "from othello import Move, Player\n",
    "\n",
    "env = othello.make()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(alpha=0.0, batch_size=32, hidden_layer_sizes=(128,),\n",
       "             learning_rate_init=0.0001, warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(alpha=0.0, batch_size=32, hidden_layer_sizes=(128,),\n",
       "             learning_rate_init=0.0001, warm_start=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(alpha=0.0, batch_size=32, hidden_layer_sizes=(128,),\n",
       "             learning_rate_init=0.0001, warm_start=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "reg = MLPRegressor(\n",
    "    hidden_layer_sizes=(128,),\n",
    "    activation=\"relu\",\n",
    "    learning_rate=\"constant\",\n",
    "    learning_rate_init=1.0e-4,\n",
    "    alpha=0.0,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    warm_start=True,\n",
    ")\n",
    "\n",
    "dummy_X = np.zeros((reg.batch_size, 8 * 8 * 2))\n",
    "dummy_y = np.zeros((reg.batch_size, 8 * 8 + 1))\n",
    "reg.partial_fit(dummy_X, dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_move_index(move):\n",
    "    if move.is_pass():\n",
    "        return 64\n",
    "    return move.y * 8 + move.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature(player, board):\n",
    "    b0 = np.maximum(0, +1.0 * player * board)\n",
    "    b1 = np.maximum(0, -1.0 * player * board)\n",
    "    feature = np.concatenate([b0.flatten(), b1.flatten()])\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def move_by_random(env, player):\n",
    "    \"\"\"有効手の中からランダムに手を選ぶ\"\"\"\n",
    "    moves = env.legal_moves(player)\n",
    "    if len(moves) == 0:\n",
    "        return Move.Pass(player)\n",
    "    else:\n",
    "        return np.random.choice(moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match():\n",
    "    # ゲームのリセット\n",
    "    env = othello.make()\n",
    "    player, board = env.reset()\n",
    "\n",
    "    # エピソード開始\n",
    "    while not env.gameset():\n",
    "        # 黒番:\n",
    "        if player == Player.BLACK:\n",
    "            # セル評価値が最も高い場所に着手する\n",
    "            moves = env.legal_moves(player)\n",
    "\n",
    "            if len(moves) == 0:\n",
    "                move = Move.Pass(player)\n",
    "            else:\n",
    "                best_move = None\n",
    "                best_score = -np.inf\n",
    "                feature = get_feature(player, board)\n",
    "                action_values = reg.predict([feature])[0]\n",
    "                for m in moves:\n",
    "                    value = action_values[get_move_index(m)]\n",
    "                    if best_score < value:\n",
    "                        best_score = value\n",
    "                        best_move = m\n",
    "\n",
    "                move = best_move\n",
    "        # 白番:\n",
    "        if player == Player.WHITE:\n",
    "            # 着手可能な手があればランダムに1つを選ぶ\n",
    "            move = move_by_random(env, player)\n",
    "\n",
    "        # 着手による盤の状態の更新\n",
    "        player, board = env.step(move)\n",
    "\n",
    "    return env.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_score(n_matches=1000, n_jobs=4, verbose=False):\n",
    "    if verbose:\n",
    "        with tqdm_joblib(n_matches):\n",
    "            result = joblib.Parallel(n_jobs=n_jobs)(\n",
    "                (joblib.delayed(match)() for _ in range(n_matches)),\n",
    "            )\n",
    "    else:\n",
    "        result = joblib.Parallel(n_jobs=n_jobs)(\n",
    "            (joblib.delayed(match)() for _ in range(n_matches)),\n",
    "        )\n",
    "        \n",
    "    result = np.array(result, dtype=\"int32\")\n",
    "    b_win = np.sum(result[:, 0] > result[:, 1])\n",
    "    w_win = np.sum(result[:, 0] < result[:, 1])\n",
    "    draw = np.sum(result[:, 0] == result[:, 1])\n",
    "\n",
    "    return b_win / n_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814d78f659cb445bbf2f959fd4683592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_episodes = 10\n",
    "n_matches = 100\n",
    "n_epochs = 100\n",
    "gamma = 0.999\n",
    "\n",
    "e0 = 0.5\n",
    "e1 = 0.1\n",
    "e_scale = np.exp((np.log(e1) - np.log(e0)) / (n_episodes - 1))\n",
    "epsilon = e0\n",
    "\n",
    "pbar = tqdm(total=n_episodes * n_epochs)\n",
    "win = 0.0\n",
    "for _ in range(n_episodes):\n",
    "    X = []\n",
    "    y = []\n",
    "    for _ in range(n_matches):\n",
    "        player, board = env.reset()\n",
    "        while not env.gameset():\n",
    "            # 入力のベクトルは現在の状態\n",
    "            feature = get_feature(player, board)\n",
    "            action_values = reg.predict([feature])[0]\n",
    "            X.append(feature.copy())\n",
    "\n",
    "            # 手を指して状態を進める\n",
    "            # 以下はε-greedy法で手を選ぶ\n",
    "            moves = env.legal_moves(player)\n",
    "            if len(moves) == 0:\n",
    "                move = Move.Pass(player)\n",
    "            elif np.random.uniform(0.0, 1.0) < epsilon:\n",
    "                move = np.random.choice(moves)\n",
    "            else:\n",
    "                best_move = None\n",
    "                best_score = -np.inf\n",
    "                for m in moves:\n",
    "                    value = action_values[get_move_index(m)]\n",
    "                    if best_score < value:\n",
    "                        best_score = value\n",
    "                        best_move = m\n",
    "                move = best_move\n",
    "\n",
    "            next_player, next_board = env.step(move)\n",
    "\n",
    "            reward = 0.0\n",
    "            if env.gameset():\n",
    "                # 対戦結果に応じて報酬(罰)を与える\n",
    "                n_now = env.count(player)\n",
    "                n_next = env.count(next_player)\n",
    "                if n_now > n_next:\n",
    "                    reward = 1.0\n",
    "                elif n_now < n_next:\n",
    "                    reward = -1.0\n",
    "                else:\n",
    "                    reward = 0.0\n",
    "\n",
    "                # 次の局面はないので, 次局面からの報酬伝搬はない\n",
    "                next_best_score = 0.0\n",
    "            else:\n",
    "                # 相手にとって最も良いQ値を得る\n",
    "                next_moves = env.legal_moves(next_player)\n",
    "                feature = get_feature(next_player, next_board)\n",
    "                next_action_values = reg.predict([feature])[0]\n",
    "\n",
    "                if len(next_moves) == 0:\n",
    "                    next_best_score = next_action_values[-1]\n",
    "                else:\n",
    "                    next_best_score = -np.inf\n",
    "                    for m in next_moves:\n",
    "                        value = next_action_values[get_move_index(m)]\n",
    "                        if next_best_score < value:\n",
    "                            next_best_score = value\n",
    "\n",
    "                # 相手の報酬は自分にとっては損なのでマイナスにする\n",
    "                next_best_score *= -1.0\n",
    "                \n",
    "            # 着手に対応する行動価値を更新\n",
    "            action_index = get_move_index(move)\n",
    "            target = action_values\n",
    "            target[action_index] = reward + gamma * next_best_score\n",
    "            y.append(target.copy())\n",
    "\n",
    "            # 変数を更新\n",
    "            player = next_player\n",
    "            board = next_board\n",
    "\n",
    "    # 1エピソードごとに回帰モデルを更新\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.stack(y, axis=0)\n",
    "    for epoch in range(n_epochs):\n",
    "        reg = reg.partial_fit(X, y)\n",
    "        pbar.set_description(f\"win={win:.3f}, loss={reg.loss_:.4f}\")\n",
    "        pbar.update()\n",
    "\n",
    "    win = win_score()\n",
    "    epsilon *= e_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b_win' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m glue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mql_b_win\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mb_win\u001b[49m, display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m glue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mql_w_win\u001b[39m\u001b[38;5;124m\"\u001b[39m, w_win, display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m glue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mql_draw\u001b[39m\u001b[38;5;124m\"\u001b[39m, draw, display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'b_win' is not defined"
     ]
    }
   ],
   "source": [
    "glue(\"ql_b_win\", b_win, display=True)\n",
    "glue(\"ql_w_win\", w_win, display=True)\n",
    "glue(\"ql_draw\", draw, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**対局結果: Q学習 vs ランダム**\n",
    "- 黒番勝ち: {glue:}`ql_b_win`\n",
    "- 白番勝ち: {glue:}`ql_w_win`\n",
    "- 引き分け: {glue:}`ql_draw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 演習内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "今回の演習では以下のようなオセロをプレイするエージェントのクラスを実装する。ただし、基本的にはオセロ環境が渡されてくる`Agent.play`だけを編集すれば良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def play(self, player, env) -> Move:\n",
    "        \"\"\"\n",
    "        この関数を更新、以下はランダムに着手する例\n",
    "        \"\"\"\n",
    "        moves = env.legal_moves()\n",
    "        if len(moves) != 0:\n",
    "            return Move.Pass(player)\n",
    "        else:\n",
    "            return np.random.choice(moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 対戦相手のレベル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- **レベル1:** セル評価値を用いたアルファベータ探索で2手先まで読むAI\n",
    "- **レベル2:** 上記のscikit-learnを用いた関数近似によって、次の手の価値を評価するAI\n",
    "- **レベル3:** Q関数の学習方法を改良し、さらにQ関数に基づいて2手先まで読むAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### ローカル環境でのテスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 本番環境でのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
